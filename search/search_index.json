{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pynector","text":"<p>Welcome to the Pynector documentation! Pynector is a Python connector library designed to simplify building robust applications that interact with external APIs and services.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Unified Transport Layer: Consistent interface for different transport   protocols</li> <li>API Client Support: Ready-to-use clients for popular services</li> <li>Concurrency Patterns: Tools for handling concurrent operations efficiently</li> <li>Observability: Built-in logging and tracing capabilities</li> <li>Error Handling: Comprehensive error handling and retry mechanisms</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Check out the Installation guide to get started with Pynector, or dive into the Quick Start guide to see it in action.</p>"},{"location":"#project-status","title":"Project Status","text":"<p>Pynector is currently in active development. APIs may change between versions.</p>"},{"location":"#license","title":"License","text":"<p>Pynector is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to Pynector will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"[Unreleased]","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Initial project structure</li> <li>Basic client implementation</li> <li>HTTP transport layer</li> <li>SDK transport layer with OpenAI and Anthropic adapters</li> <li>Concurrency utilities and patterns</li> <li>Telemetry system with logging and tracing</li> <li>Comprehensive error handling system</li> <li>Documentation system using MkDocs with Material theme</li> </ul>"},{"location":"changelog/#010-2025-05-05","title":"[0.1.0] - 2025-05-05","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Initial release with core functionality</li> <li>HTTP transport implementation</li> <li>Basic client interface</li> <li>Concurrency primitives</li> <li>Error handling framework</li> </ul>"},{"location":"client/","title":"Core Pynector Client","text":"<p>The <code>Pynector</code> class is the main entry point for using the Pynector library. It provides a high-level interface that integrates the Transport Abstraction Layer, Structured Concurrency, and Optional Observability components into a cohesive, user-friendly API.</p>"},{"location":"client/#key-features","title":"Key Features","text":"<ul> <li>Flexible Transport Integration: Works with both built-in and custom   transports</li> <li>Efficient Batch Processing: Parallel request processing with concurrency   limits</li> <li>Optional Observability: Integrated tracing and logging with no-op   fallbacks</li> <li>Resource Safety: Proper async resource management with context managers</li> <li>Robust Error Handling: Specific exception types and retry mechanisms</li> <li>Configuration Hierarchy: Supports instance config, environment variables,   and defaults</li> </ul>"},{"location":"client/#installation","title":"Installation","text":"<pre><code># Basic installation\npip install pynector\n\n# With observability features\npip install pynector[observability]\n</code></pre>"},{"location":"client/#basic-usage","title":"Basic Usage","text":""},{"location":"client/#creating-a-client","title":"Creating a Client","text":"<pre><code>from pynector import Pynector\n\n# Create a client with default HTTP transport\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    headers={\"Content-Type\": \"application/json\"}\n)\n\n# Using as an async context manager (recommended)\nasync with Pynector(transport_type=\"http\", base_url=\"https://api.example.com\") as client:\n    # Client is automatically connected and will be properly closed when exiting the context\n    response = await client.request({\"path\": \"/users\", \"method\": \"GET\"})\n</code></pre>"},{"location":"client/#making-requests","title":"Making Requests","text":"<pre><code># Simple request\nresponse = await client.request(\n    {\"path\": \"/users\", \"method\": \"GET\", \"params\": {\"limit\": 10}}\n)\n\n# Request with timeout\ntry:\n    response = await client.request(\n        {\"path\": \"/users\", \"method\": \"GET\"},\n        timeout=5.0  # 5 second timeout\n    )\nexcept TimeoutError:\n    print(\"Request timed out\")\n\n# Request with retry for transient errors\nresponse = await client.request_with_retry(\n    {\"path\": \"/users\", \"method\": \"GET\"},\n    max_retries=3,\n    retry_delay=1.0  # Initial delay, will increase exponentially\n)\n</code></pre>"},{"location":"client/#batch-requests","title":"Batch Requests","text":"<pre><code># Create a batch of requests\nrequests = [\n    ({\"path\": \"/users/1\", \"method\": \"GET\"}, {}),\n    ({\"path\": \"/users/2\", \"method\": \"GET\"}, {}),\n    ({\"path\": \"/users/3\", \"method\": \"GET\"}, {})\n]\n\n# Process requests in parallel with concurrency limit\nresponses = await client.batch_request(\n    requests,\n    max_concurrency=2,  # Process at most 2 requests at a time\n    timeout=10.0,       # 10 second timeout for the entire batch\n    raise_on_error=False  # Return exceptions instead of raising them\n)\n\n# Check results\nfor i, response in enumerate(responses):\n    if isinstance(response, Exception):\n        print(f\"Request {i} failed: {response}\")\n    else:\n        print(f\"Request {i} succeeded: {response}\")\n</code></pre>"},{"location":"client/#resource-management","title":"Resource Management","text":"<pre><code># Manual resource management\nclient = Pynector(transport_type=\"http\", base_url=\"https://api.example.com\")\ntry:\n    response = await client.request({\"path\": \"/users\", \"method\": \"GET\"})\nfinally:\n    await client.aclose()  # Ensure resources are properly released\n\n# Using async context manager (recommended)\nasync with Pynector(transport_type=\"http\", base_url=\"https://api.example.com\") as client:\n    response = await client.request({\"path\": \"/users\", \"method\": \"GET\"})\n    # Resources automatically released when exiting the context\n</code></pre>"},{"location":"client/#advanced-usage","title":"Advanced Usage","text":""},{"location":"client/#custom-configuration","title":"Custom Configuration","text":"<pre><code># Instance configuration\nclient = Pynector(\n    transport_type=\"http\",\n    config={\n        \"timeout\": 30.0,\n        \"retry_count\": 3,\n        \"max_connections\": 10\n    }\n)\n\n# Environment variables (set before creating client)\n# PYNECTOR_TIMEOUT=30.0\n# PYNECTOR_RETRY_COUNT=3\nclient = Pynector(transport_type=\"http\")\n# Will use environment variables for configuration\n</code></pre>"},{"location":"client/#using-a-custom-transport","title":"Using a Custom Transport","text":"<pre><code>from pynector.transport.protocol import Transport\n\n# Create a custom transport implementation\nclass MyCustomTransport(Transport):\n    async def connect(self) -&gt; None:\n        # Implementation\n        pass\n\n    async def disconnect(self) -&gt; None:\n        # Implementation\n        pass\n\n    async def send(self, data, **options) -&gt; None:\n        # Implementation\n        pass\n\n    async def receive(self):\n        # Implementation\n        yield b\"response data\"\n\n# Use the custom transport\ncustom_transport = MyCustomTransport()\nclient = Pynector(transport=custom_transport)\n</code></pre>"},{"location":"client/#integrating-with-telemetry","title":"Integrating with Telemetry","text":"<pre><code># Configure telemetry (optional)\nfrom pynector.telemetry import configure_telemetry\n\nconfigure_telemetry(\n    service_name=\"my-service\",\n    log_level=\"INFO\"\n)\n\n# Create client with telemetry enabled\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    enable_telemetry=True  # Default is True\n)\n\n# Telemetry will automatically capture spans and logs for requests\n</code></pre>"},{"location":"client/#http-transport-specific-options","title":"HTTP Transport Specific Options","text":"<pre><code># Create client with HTTP transport specific options\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    headers={\"Authorization\": \"Bearer token\"},\n    timeout=30.0,\n    follow_redirects=True,\n    verify=True,  # Verify SSL certificates\n    proxies={\"http\": \"http://proxy.example.com:8080\"}\n)\n</code></pre>"},{"location":"client/#sdk-transport-specific-options","title":"SDK Transport Specific Options","text":"<pre><code># Create client with SDK transport for OpenAI\nclient = Pynector(\n    transport_type=\"sdk\",\n    provider=\"openai\",\n    api_key=\"your-api-key\",\n    organization=\"your-org-id\"  # Optional\n)\n\n# Create client with SDK transport for Anthropic\nclient = Pynector(\n    transport_type=\"sdk\",\n    provider=\"anthropic\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"client/#error-handling","title":"Error Handling","text":"<p>The Pynector client provides a comprehensive error hierarchy for handling different types of errors:</p> <pre><code>from pynector.errors import (\n    PynectorError,       # Base exception for all Pynector errors\n    ConfigurationError,  # Error in client or transport configuration\n    TransportError,      # Error in transport layer (e.g., connection error)\n    TimeoutError,        # Request timed out\n)\n\ntry:\n    response = await client.request({\"path\": \"/users\", \"method\": \"GET\"})\nexcept TimeoutError:\n    print(\"Request timed out\")\nexcept TransportError as e:\n    print(f\"Transport error: {e}\")\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\nexcept PynectorError as e:\n    print(f\"Other Pynector error: {e}\")\n</code></pre>"},{"location":"client/#api-reference","title":"API Reference","text":""},{"location":"client/#pynector-class","title":"Pynector Class","text":"<pre><code>class Pynector:\n    def __init__(\n        self,\n        transport: Optional[Transport] = None,\n        transport_type: str = \"http\",\n        enable_telemetry: bool = True,\n        config: Optional[dict[str, Any]] = None,\n        **transport_options,\n    ):\n        \"\"\"Initialize the Pynector instance.\n\n        Args:\n            transport: Optional pre-configured transport instance to use.\n            transport_type: Type of transport to create if transport is not provided.\n            enable_telemetry: Whether to enable telemetry features.\n            config: Configuration options for the client.\n            **transport_options: Additional options passed to the transport factory.\n        \"\"\"\n        # ...\n\n    async def request(\n        self, data: Any, timeout: Optional[float] = None, **options\n    ) -&gt; Any:\n        \"\"\"Send a single request and return the response.\n\n        Args:\n            data: The data to send.\n            timeout: Optional timeout in seconds for this specific request.\n            **options: Additional options for the request.\n\n        Returns:\n            The response data.\n\n        Raises:\n            TransportError: If there is an error with the transport.\n            TimeoutError: If the request times out.\n            PynectorError: For other errors.\n        \"\"\"\n        # ...\n\n    async def batch_request(\n        self,\n        requests: list[tuple[Any, dict]],\n        max_concurrency: Optional[int] = None,\n        timeout: Optional[float] = None,\n        raise_on_error: bool = False,\n        **options,\n    ) -&gt; list[Any]:\n        \"\"\"Send multiple requests in parallel and return the responses.\n\n        Args:\n            requests: List of (data, options) tuples.\n            max_concurrency: Maximum number of concurrent requests.\n            timeout: Optional timeout in seconds for the entire batch.\n            raise_on_error: Whether to raise on the first error.\n            **options: Additional options for all requests.\n\n        Returns:\n            List of responses or exceptions.\n\n        Raises:\n            TimeoutError: If the batch times out and raise_on_error is True.\n            PynectorError: For other errors if raise_on_error is True.\n        \"\"\"\n        # ...\n\n    async def request_with_retry(\n        self, data: Any, max_retries: int = 3, retry_delay: float = 1.0, **options\n    ) -&gt; Any:\n        \"\"\"Send a request with retry for transient errors.\n\n        Args:\n            data: The data to send\n            max_retries: The maximum number of retry attempts\n            retry_delay: The initial delay between retries (will be exponentially increased)\n            **options: Additional options for the request\n\n        Returns:\n            The response data\n\n        Raises:\n            TransportError: If all retry attempts fail\n            TimeoutError: If the request times out after all retry attempts\n            PynectorError: For other errors\n        \"\"\"\n        # ...\n\n    async def aclose(self) -&gt; None:\n        \"\"\"Close the Pynector instance and release resources.\"\"\"\n        # ...\n\n    async def __aenter__(self) -&gt; \"Pynector\":\n        \"\"\"Enter the async context.\"\"\"\n        # ...\n\n    async def __aexit__(\n        self,\n        exc_type: Optional[type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -&gt; None:\n        \"\"\"Exit the async context.\"\"\"\n        # ...\n</code></pre>"},{"location":"client/#performance-considerations","title":"Performance Considerations","text":""},{"location":"client/#concurrency-limits","title":"Concurrency Limits","text":"<p>When using <code>batch_request</code>, the <code>max_concurrency</code> parameter controls how many requests can be processed concurrently. This is important for:</p> <ol> <li>Resource Management: Prevents overwhelming the system with too many    concurrent connections</li> <li>Rate Limiting: Helps stay within API rate limits</li> <li>Performance Optimization: Finding the optimal concurrency level for your    specific workload</li> </ol> <pre><code># Example: Finding optimal concurrency\nimport time\nimport asyncio\nfrom pynector import Pynector\n\nasync def benchmark_concurrency():\n    client = Pynector(transport_type=\"http\", base_url=\"https://api.example.com\")\n\n    # Create a batch of 100 requests\n    requests = [({\"path\": \"/test\", \"method\": \"GET\"}, {}) for _ in range(100)]\n\n    # Test different concurrency limits\n    concurrency_limits = [1, 5, 10, 20, 50, 100]\n    results = []\n\n    for limit in concurrency_limits:\n        start_time = time.time()\n        await client.batch_request(requests, max_concurrency=limit)\n        duration = time.time() - start_time\n        results.append((limit, duration))\n        print(f\"Concurrency {limit}: {duration:.2f} seconds\")\n\n    await client.aclose()\n    return results\n\nasyncio.run(benchmark_concurrency())\n</code></pre>"},{"location":"client/#timeout-handling","title":"Timeout Handling","text":"<p>Proper timeout handling is crucial for preventing resource leaks and ensuring responsiveness:</p> <pre><code># Global timeout in configuration\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    config={\"timeout\": 10.0}  # Global default timeout\n)\n\n# Per-request timeout (overrides global)\nresponse = await client.request(\n    {\"path\": \"/users\", \"method\": \"GET\"},\n    timeout=5.0  # This request has a 5-second timeout\n)\n\n# Batch request timeout (applies to the entire batch)\nresponses = await client.batch_request(\n    requests,\n    timeout=15.0  # The entire batch must complete within 15 seconds\n)\n</code></pre>"},{"location":"client/#integration-with-other-components","title":"Integration with Other Components","text":"<p>The Pynector client integrates seamlessly with other components of the library:</p>"},{"location":"client/#transport-abstraction-layer","title":"Transport Abstraction Layer","text":"<pre><code># HTTP Transport\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    headers={\"Content-Type\": \"application/json\"}\n)\n\n# SDK Transport\nclient = Pynector(\n    transport_type=\"sdk\",\n    provider=\"openai\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"client/#structured-concurrency","title":"Structured Concurrency","text":"<p>The client uses AnyIO's structured concurrency primitives internally for:</p> <ul> <li>Task groups in batch requests</li> <li>Timeout handling with cancellation scopes</li> <li>Concurrency limiting with capacity limiters</li> </ul>"},{"location":"client/#optional-observability","title":"Optional Observability","text":"<pre><code># Configure telemetry\nfrom pynector.telemetry import configure_telemetry\n\nconfigure_telemetry(\n    service_name=\"my-service\",\n    log_level=\"INFO\"\n)\n\n# Create client with telemetry\nclient = Pynector(\n    transport_type=\"http\",\n    base_url=\"https://api.example.com\",\n    enable_telemetry=True\n)\n\n# Each request will automatically:\n# - Create a span with request/response attributes\n# - Log request start/completion/errors\n# - Propagate trace context\n</code></pre>"},{"location":"concurrency/","title":"Structured Concurrency","text":"<p>The Structured Concurrency module is a core component of Pynector that provides a robust foundation for managing concurrent operations in Python. It leverages AnyIO to provide a consistent interface for structured concurrency across both asyncio and trio backends, focusing on task groups, cancellation scopes, and resource management primitives.</p>"},{"location":"concurrency/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Design Philosophy</li> <li>Components</li> <li>Task Groups</li> <li>Cancellation Scopes</li> <li>Resource Management Primitives</li> <li>Error Handling</li> <li>Concurrency Patterns</li> <li>Usage Examples</li> <li>Basic Task Group Usage</li> <li>Cancellation and Timeouts</li> <li>Resource Management</li> <li>Using Concurrency Patterns</li> <li>Error Handling</li> </ul>"},{"location":"concurrency/#design-philosophy","title":"Design Philosophy","text":"<p>The Structured Concurrency module is designed with the following principles in mind:</p>"},{"location":"concurrency/#structured-concurrency-pattern","title":"Structured Concurrency Pattern","text":"<p>Structured concurrency is a programming pattern that ensures that concurrent operations have well-defined lifetimes that are bound to a scope. This means that:</p> <ul> <li>Scope-Bound Lifetimes: All tasks started in a scope are guaranteed to   complete before the scope exits.</li> <li>Automatic Cleanup: Resources are automatically cleaned up when the scope   exits.</li> <li>Error Propagation: Errors from any task are propagated to the parent   scope.</li> </ul> <p>This pattern makes concurrent code more predictable, easier to reason about, and less prone to resource leaks and race conditions.</p>"},{"location":"concurrency/#anyio-integration","title":"AnyIO Integration","text":"<p>The module uses AnyIO as its foundation, which provides a consistent interface for both asyncio and trio backends. This allows developers to write code that works with either backend without modification.</p>"},{"location":"concurrency/#context-management","title":"Context Management","text":"<p>The module uses async context managers extensively for resource handling. This ensures that resources are properly acquired and released, even in the presence of exceptions.</p>"},{"location":"concurrency/#comprehensive-error-handling","title":"Comprehensive Error Handling","text":"<p>The module provides a comprehensive error handling system that makes it easier to handle specific error conditions and propagate errors correctly.</p>"},{"location":"concurrency/#components","title":"Components","text":""},{"location":"concurrency/#task-groups","title":"Task Groups","text":"<p>Task groups provide a way to spawn and manage multiple concurrent tasks while ensuring proper cleanup and error propagation.</p> <pre><code>from pynector.concurrency import TaskGroup, create_task_group\n\nasync def main():\n    async with create_task_group() as tg:\n        await tg.start_soon(task1)\n        await tg.start_soon(task2)\n        # All tasks will complete before exiting the context\n</code></pre> <p>The <code>TaskGroup</code> class provides the following methods:</p> <ul> <li><code>start_soon(func, *args, name=None)</code>: Start a new task in the task group   without waiting for it to initialize.</li> <li><code>start(func, *args, name=None)</code>: Start a new task and wait for it to   initialize.</li> </ul>"},{"location":"concurrency/#cancellation-scopes","title":"Cancellation Scopes","text":"<p>Cancellation scopes provide fine-grained control over task cancellation and timeouts.</p> <pre><code>from pynector.concurrency import CancelScope, move_on_after, fail_after\n\nasync def main():\n    # Using CancelScope directly\n    with CancelScope() as scope:\n        # Do something\n        if condition:\n            scope.cancel()  # Cancel all tasks in this scope\n\n    # Using timeout utilities\n    with move_on_after(5) as scope:  # Continue after 5 seconds\n        await long_running_operation()\n        if scope.cancelled_caught:\n            print(\"Operation timed out\")\n\n    # Using fail_after to raise TimeoutError\n    try:\n        with fail_after(5):  # Raise TimeoutError after 5 seconds\n            await long_running_operation()\n    except TimeoutError:\n        print(\"Operation timed out\")\n</code></pre>"},{"location":"concurrency/#resource-management-primitives","title":"Resource Management Primitives","text":"<p>The module provides several primitives for managing concurrent access to resources:</p>"},{"location":"concurrency/#lock","title":"Lock","text":"<p>A mutex lock for controlling access to a shared resource.</p> <pre><code>from pynector.concurrency import Lock\n\nasync def main():\n    lock = Lock()\n\n    async with lock:\n        # Critical section\n        # Only one task can execute this at a time\n</code></pre>"},{"location":"concurrency/#semaphore","title":"Semaphore","text":"<p>A semaphore for limiting concurrent access to a resource.</p> <pre><code>from pynector.concurrency import Semaphore\n\nasync def main():\n    semaphore = Semaphore(3)  # Allow up to 3 concurrent accesses\n\n    async with semaphore:\n        # Up to 3 tasks can execute this concurrently\n</code></pre>"},{"location":"concurrency/#capacitylimiter","title":"CapacityLimiter","text":"<p>A context manager for limiting the number of concurrent operations.</p> <pre><code>from pynector.concurrency import CapacityLimiter\n\nasync def main():\n    limiter = CapacityLimiter(10)  # Allow up to 10 concurrent operations\n\n    async with limiter:\n        # Up to 10 tasks can execute this concurrently\n</code></pre>"},{"location":"concurrency/#event","title":"Event","text":"<p>An event object for task synchronization.</p> <pre><code>from pynector.concurrency import Event\n\nasync def main():\n    event = Event()\n\n    # In one task\n    await event.wait()  # Wait until the event is set\n\n    # In another task\n    event.set()  # Allow waiting tasks to proceed\n</code></pre>"},{"location":"concurrency/#condition","title":"Condition","text":"<p>A condition variable for task synchronization.</p> <pre><code>from pynector.concurrency import Condition, Lock\n\nasync def main():\n    condition = Condition(Lock())\n\n    async with condition:\n        await condition.wait()  # Wait for a notification\n\n    async with condition:\n        await condition.notify()  # Notify one waiting task\n        # or\n        await condition.notify_all()  # Notify all waiting tasks\n</code></pre>"},{"location":"concurrency/#error-handling","title":"Error Handling","text":"<p>The module provides utilities for handling cancellation and shielding tasks from cancellation.</p> <pre><code>from pynector.concurrency import get_cancelled_exc_class, shield\n\nasync def main():\n    # Get the exception class used for cancellation\n    cancelled_exc_class = get_cancelled_exc_class()\n\n    try:\n        # Do something\n        pass\n    except cancelled_exc_class:\n        print(\"Task was cancelled\")\n\n    # Shield a task from cancellation\n    result = await shield(critical_operation)\n</code></pre>"},{"location":"concurrency/#concurrency-patterns","title":"Concurrency Patterns","text":"<p>The module implements several common concurrency patterns:</p>"},{"location":"concurrency/#connectionpool","title":"ConnectionPool","text":"<p>A pool of reusable connections.</p> <pre><code>from pynector.concurrency.patterns import ConnectionPool\n\nasync def create_connection():\n    # Create and return a connection\n    pass\n\nasync def main():\n    pool = ConnectionPool(max_connections=10, connection_factory=create_connection)\n\n    async with pool as p:\n        # Acquire a connection\n        conn = await p.acquire()\n\n        try:\n            # Use the connection\n            pass\n        finally:\n            # Release the connection back to the pool\n            await p.release(conn)\n</code></pre>"},{"location":"concurrency/#parallel-requests","title":"Parallel Requests","text":"<p>Fetch multiple URLs in parallel with limited concurrency.</p> <pre><code>from pynector.concurrency.patterns import parallel_requests\n\nasync def fetch(url):\n    # Fetch and return the response\n    pass\n\nasync def main():\n    urls = [\"https://example.com\", \"https://example.org\", \"https://example.net\"]\n    responses = await parallel_requests(urls, fetch, max_concurrency=5)\n\n    for response in responses:\n        print(response)\n</code></pre>"},{"location":"concurrency/#retry-with-timeout","title":"Retry with Timeout","text":"<p>Execute a function with retry logic and timeout.</p> <pre><code>from pynector.concurrency.patterns import retry_with_timeout\n\nasync def flaky_operation():\n    # An operation that might fail or time out\n    pass\n\nasync def main():\n    try:\n        result = await retry_with_timeout(\n            flaky_operation,\n            max_retries=3,\n            timeout=5.0,\n            retry_exceptions=[ConnectionError, TimeoutError]\n        )\n    except TimeoutError:\n        print(\"Operation timed out after all retries\")\n    except Exception as e:\n        print(f\"Operation failed: {e}\")\n</code></pre>"},{"location":"concurrency/#worker-pool","title":"Worker Pool","text":"<p>A pool of worker tasks that process items from a queue.</p> <pre><code>from pynector.concurrency.patterns import WorkerPool\n\nasync def process_item(item):\n    # Process the item\n    pass\n\nasync def main():\n    pool = WorkerPool(num_workers=5, worker_func=process_item)\n\n    await pool.start()\n\n    try:\n        # Submit items for processing\n        for item in items:\n            await pool.submit(item)\n    finally:\n        # Stop the worker pool\n        await pool.stop()\n</code></pre>"},{"location":"concurrency/#usage-examples","title":"Usage Examples","text":""},{"location":"concurrency/#basic-task-group-usage","title":"Basic Task Group Usage","text":"<p>Here's a basic example of how to use task groups:</p> <pre><code>import asyncio\nfrom pynector.concurrency import create_task_group\n\nasync def task1():\n    print(\"Task 1 started\")\n    await asyncio.sleep(1)\n    print(\"Task 1 completed\")\n\nasync def task2():\n    print(\"Task 2 started\")\n    await asyncio.sleep(2)\n    print(\"Task 2 completed\")\n\nasync def main():\n    async with create_task_group() as tg:\n        await tg.start_soon(task1)\n        await tg.start_soon(task2)\n        print(\"All tasks started\")\n\n    print(\"All tasks completed\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"concurrency/#cancellation-and-timeouts","title":"Cancellation and Timeouts","text":"<p>Here's an example of how to use cancellation scopes and timeouts:</p> <pre><code>import asyncio\nfrom pynector.concurrency import move_on_after, fail_after\n\nasync def long_running_task():\n    print(\"Long-running task started\")\n    await asyncio.sleep(10)\n    print(\"Long-running task completed\")\n\nasync def main():\n    # Using move_on_after\n    print(\"Using move_on_after:\")\n    with move_on_after(2) as scope:\n        await long_running_task()\n        if scope.cancelled_caught:\n            print(\"Task was cancelled after timeout\")\n\n    print(\"Continued execution after timeout\")\n\n    # Using fail_after\n    print(\"\\nUsing fail_after:\")\n    try:\n        with fail_after(2):\n            await long_running_task()\n    except TimeoutError:\n        print(\"TimeoutError was raised\")\n\n    print(\"Continued execution after timeout exception\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"concurrency/#resource-management","title":"Resource Management","text":"<p>Here's an example of how to use resource management primitives:</p> <pre><code>import asyncio\nfrom pynector.concurrency import Lock, Semaphore, CapacityLimiter\n\nshared_resource = []\n\nasync def task_with_lock(lock, item):\n    async with lock:\n        print(f\"Adding {item} to shared resource\")\n        shared_resource.append(item)\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"Finished processing {item}\")\n\nasync def task_with_semaphore(semaphore, item):\n    async with semaphore:\n        print(f\"Processing {item} with semaphore\")\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"Finished processing {item}\")\n\nasync def task_with_limiter(limiter, item):\n    async with limiter:\n        print(f\"Processing {item} with limiter\")\n        await asyncio.sleep(1)  # Simulate work\n        print(f\"Finished processing {item}\")\n\nasync def main():\n    # Using Lock\n    lock = Lock()\n    tasks = [task_with_lock(lock, i) for i in range(5)]\n    await asyncio.gather(*tasks)\n    print(f\"Shared resource: {shared_resource}\\n\")\n\n    # Using Semaphore\n    semaphore = Semaphore(3)  # Allow up to 3 concurrent tasks\n    tasks = [task_with_semaphore(semaphore, i) for i in range(5)]\n    await asyncio.gather(*tasks)\n    print()\n\n    # Using CapacityLimiter\n    limiter = CapacityLimiter(2)  # Allow up to 2 concurrent tasks\n    tasks = [task_with_limiter(limiter, i) for i in range(5)]\n    await asyncio.gather(*tasks)\n\nasyncio.run(main())\n</code></pre>"},{"location":"concurrency/#using-concurrency-patterns","title":"Using Concurrency Patterns","text":"<p>Here's an example of how to use the concurrency patterns:</p> <pre><code>import asyncio\nfrom pynector.concurrency.patterns import parallel_requests, retry_with_timeout\n\nasync def fetch(url):\n    print(f\"Fetching {url}\")\n    await asyncio.sleep(1)  # Simulate network delay\n    return f\"Response from {url}\"\n\nasync def flaky_operation():\n    import random\n    if random.random() &lt; 0.7:  # 70% chance of failure\n        raise ConnectionError(\"Simulated connection error\")\n    return \"Success\"\n\nasync def main():\n    # Using parallel_requests\n    urls = [\"https://example.com\", \"https://example.org\", \"https://example.net\"]\n    responses = await parallel_requests(urls, fetch, max_concurrency=2)\n\n    for url, response in zip(urls, responses):\n        print(f\"{url} -&gt; {response}\")\n\n    print()\n\n    # Using retry_with_timeout\n    try:\n        result = await retry_with_timeout(\n            flaky_operation,\n            max_retries=5,\n            timeout=1.0,\n            retry_exceptions=[ConnectionError]\n        )\n        print(f\"Operation succeeded: {result}\")\n    except TimeoutError:\n        print(\"Operation timed out after all retries\")\n    except Exception as e:\n        print(f\"Operation failed: {e}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"concurrency/#error-handling-examples","title":"Error Handling Examples","text":"<p>Here's an example of how to handle errors in structured concurrency:</p> <pre><code>import asyncio\nfrom pynector.concurrency import create_task_group, get_cancelled_exc_class, shield\n\nasync def task_that_fails():\n    await asyncio.sleep(1)\n    raise ValueError(\"Simulated error\")\n\nasync def task_that_gets_cancelled():\n    try:\n        await asyncio.sleep(10)\n    except get_cancelled_exc_class():\n        print(\"Task was cancelled\")\n        raise\n\nasync def critical_operation():\n    print(\"Starting critical operation\")\n    await asyncio.sleep(1)\n    print(\"Critical operation completed\")\n    return \"Critical result\"\n\nasync def main():\n    # Handling errors in task groups\n    try:\n        async with create_task_group() as tg:\n            await tg.start_soon(task_that_fails)\n            await tg.start_soon(asyncio.sleep, 2)\n    except ValueError as e:\n        print(f\"Caught error from task: {e}\")\n\n    # Handling cancellation\n    try:\n        async with create_task_group() as tg:\n            await tg.start_soon(task_that_gets_cancelled)\n            await asyncio.sleep(0.5)\n            # The task group will be cancelled when we exit the context\n    except Exception as e:\n        print(f\"Caught exception: {e}\")\n\n    # Shielding from cancellation\n    try:\n        async with create_task_group() as tg:\n            await tg.start_soon(task_that_gets_cancelled)\n            # Shield the critical operation from cancellation\n            result = await shield(critical_operation)\n            print(f\"Got result: {result}\")\n            await asyncio.sleep(0.5)\n            # The task group will be cancelled when we exit the context\n    except Exception as e:\n        print(f\"Caught exception: {e}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"http_transport/","title":"HTTP Transport Implementation","text":"<p>The HTTP Transport Implementation is a concrete implementation of the Transport Abstraction Layer for HTTP communication. It provides a robust, feature-rich HTTP client based on the <code>httpx</code> library, enabling efficient and reliable HTTP communication with support for modern HTTP features.</p>"},{"location":"http_transport/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Components</li> <li>HTTPTransport</li> <li>HttpMessage</li> <li>HTTPTransportFactory</li> <li>HTTP Error Hierarchy</li> <li>Features</li> <li>Connection Pooling</li> <li>Retry Mechanism</li> <li>Error Handling</li> <li>HTTP Features Support</li> <li>Streaming Support</li> <li>Usage Examples</li> <li>Basic HTTP GET Request</li> <li>HTTP POST with JSON Data</li> <li>File Upload</li> <li>Custom Headers and Authentication</li> <li>Handling Errors</li> <li>Streaming Responses</li> <li>Configuring Retries</li> </ul>"},{"location":"http_transport/#overview","title":"Overview","text":"<p>The HTTP Transport Implementation provides a complete solution for HTTP communication within the Pynector framework. It follows the Transport Protocol defined in the Transport Abstraction Layer, ensuring compatibility with the rest of the framework while providing HTTP-specific functionality.</p> <p>Key benefits of the HTTP Transport Implementation include:</p> <ul> <li>Async-first design: Built on <code>httpx.AsyncClient</code> for efficient   asynchronous HTTP communication</li> <li>Connection pooling: Reuses connections for improved performance</li> <li>Comprehensive error handling: Maps HTTP errors to the Transport error   hierarchy</li> <li>Retry mechanism: Automatically retries failed requests with exponential   backoff</li> <li>Support for modern HTTP features: Includes query parameters, headers, form   data, JSON, and file uploads</li> <li>Streaming support: Efficiently handles large responses with streaming</li> </ul>"},{"location":"http_transport/#components","title":"Components","text":""},{"location":"http_transport/#httptransport","title":"HTTPTransport","text":"<p>The <code>HTTPTransport</code> class is the core component of the HTTP Transport Implementation. It implements the Transport Protocol and provides methods for connecting, disconnecting, sending, and receiving HTTP messages.</p> <pre><code>class HTTPTransport(Transport[T], Generic[T]):\n    \"\"\"HTTP transport implementation using httpx.AsyncClient.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str = \"\",\n        headers: Optional[dict[str, str]] = None,\n        timeout: Union[float, httpx.Timeout] = 10.0,\n        max_retries: int = 3,\n        retry_backoff_factor: float = 0.5,\n        retry_status_codes: Optional[set[int]] = None,\n        follow_redirects: bool = True,\n        verify_ssl: bool = True,\n        http2: bool = False,\n    ):\n        \"\"\"Initialize the transport with configuration options.\"\"\"\n        ...\n</code></pre> <p>The <code>HTTPTransport</code> class provides the following methods:</p> <ul> <li><code>connect()</code>: Establishes the connection by initializing the AsyncClient</li> <li><code>disconnect()</code>: Closes the connection by closing the AsyncClient</li> <li><code>send(message)</code>: Sends an HTTP message</li> <li><code>receive()</code>: Receives HTTP responses</li> <li><code>stream_response(message)</code>: Streams a response from the HTTP transport</li> </ul>"},{"location":"http_transport/#httpmessage","title":"HttpMessage","text":"<p>The <code>HttpMessage</code> class implements the Message Protocol for HTTP communication. It handles serialization and deserialization of HTTP messages, including headers, payload, and binary content.</p> <pre><code>class HttpMessage:\n    \"\"\"HTTP message implementation.\"\"\"\n\n    content_type: ClassVar[str] = \"application/json\"\n\n    def __init__(\n        self,\n        method: str = \"GET\",\n        url: str = \"\",\n        headers: Optional[dict[str, str]] = None,\n        params: Optional[dict[str, Any]] = None,\n        json_data: Optional[Any] = None,\n        form_data: Optional[dict[str, Any]] = None,\n        files: Optional[dict[str, Any]] = None,\n        content: Optional[Union[str, bytes]] = None,\n    ):\n        \"\"\"Initialize an HTTP message.\"\"\"\n        ...\n</code></pre> <p>The <code>HttpMessage</code> class provides the following methods:</p> <ul> <li><code>serialize()</code>: Converts the message to bytes for transmission</li> <li><code>deserialize(data)</code>: Creates a message from received bytes</li> <li><code>get_headers()</code>: Gets the message headers</li> <li><code>get_payload()</code>: Gets the message payload</li> </ul>"},{"location":"http_transport/#httptransportfactory","title":"HTTPTransportFactory","text":"<p>The <code>HTTPTransportFactory</code> class implements the TransportFactory Protocol for creating HTTP transport instances. It follows the Factory Method pattern, providing a way to create HTTP transport instances with default configuration.</p> <pre><code>class HTTPTransportFactory:\n    \"\"\"Factory for creating HTTP transport instances.\"\"\"\n\n    def __init__(\n        self,\n        base_url: str,\n        message_type: type[T],\n        default_headers: Optional[dict[str, str]] = None,\n        default_timeout: float = 30.0,\n        default_max_retries: int = 3,\n        default_retry_backoff_factor: float = 0.5,\n        default_retry_status_codes: Optional[set[int]] = None,\n        default_follow_redirects: bool = True,\n        default_verify_ssl: bool = True,\n        default_http2: bool = False,\n    ):\n        \"\"\"Initialize the factory with default configuration.\"\"\"\n        ...\n</code></pre> <p>The <code>HTTPTransportFactory</code> class provides the following method:</p> <ul> <li><code>create_transport(**kwargs)</code>: Creates a new HTTP transport instance with the   specified configuration</li> </ul>"},{"location":"http_transport/#http-error-hierarchy","title":"HTTP Error Hierarchy","text":"<p>The HTTP Transport Implementation defines a comprehensive error hierarchy for HTTP-specific errors. This makes it easier to handle specific error conditions.</p> <pre><code>TransportError\n\u2514\u2500\u2500 TransportSpecificError\n    \u2514\u2500\u2500 HTTPTransportError\n        \u251c\u2500\u2500 HTTPStatusError\n        \u2502   \u251c\u2500\u2500 HTTPClientError\n        \u2502   \u2502   \u251c\u2500\u2500 HTTPUnauthorizedError (401)\n        \u2502   \u2502   \u251c\u2500\u2500 HTTPForbiddenError (403)\n        \u2502   \u2502   \u251c\u2500\u2500 HTTPNotFoundError (404)\n        \u2502   \u2502   \u251c\u2500\u2500 HTTPTimeoutError (408)\n        \u2502   \u2502   \u2514\u2500\u2500 HTTPTooManyRequestsError (429)\n        \u2502   \u2514\u2500\u2500 HTTPServerError (5xx)\n        \u2514\u2500\u2500 CircuitOpenError\n</code></pre> <ul> <li>HTTPTransportError: Base class for HTTP transport-specific errors</li> <li>HTTPStatusError: Error representing an HTTP response status error</li> <li>HTTPClientError: HTTP client error (4xx)<ul> <li>HTTPUnauthorizedError: HTTP unauthorized error (401)</li> <li>HTTPForbiddenError: HTTP forbidden error (403)</li> <li>HTTPNotFoundError: HTTP not found error (404)</li> <li>HTTPTimeoutError: HTTP timeout error (408)</li> <li>HTTPTooManyRequestsError: HTTP too many requests error (429)</li> </ul> </li> <li>HTTPServerError: HTTP server error (5xx)</li> <li>CircuitOpenError: Error indicating that the circuit breaker is open</li> </ul>"},{"location":"http_transport/#features","title":"Features","text":""},{"location":"http_transport/#connection-pooling","title":"Connection Pooling","text":"<p>The HTTP Transport Implementation uses <code>httpx.AsyncClient</code> for connection pooling. This means that connections are reused for multiple requests to the same host, improving performance by reducing the overhead of establishing new connections.</p> <p>Connection pooling is handled automatically by the <code>HTTPTransport</code> class, which maintains a single <code>AsyncClient</code> instance for the lifetime of the transport.</p>"},{"location":"http_transport/#retry-mechanism","title":"Retry Mechanism","text":"<p>The HTTP Transport Implementation includes a configurable retry mechanism with exponential backoff. This means that failed requests are automatically retried with increasing delays between attempts, improving reliability in the face of transient errors.</p> <p>The retry mechanism is configured with the following parameters:</p> <ul> <li><code>max_retries</code>: Maximum number of retry attempts (default: 3)</li> <li><code>retry_backoff_factor</code>: Factor for exponential backoff (default: 0.5)</li> <li><code>retry_status_codes</code>: HTTP status codes that should trigger a retry (default:   429, 500, 502, 503, 504)</li> </ul> <p>The retry mechanism also handles network-related errors, such as connection errors and timeouts.</p>"},{"location":"http_transport/#error-handling","title":"Error Handling","text":"<p>The HTTP Transport Implementation includes comprehensive error handling, mapping HTTP errors to the Transport error hierarchy. This makes it easier to handle specific error conditions.</p> <p>HTTP status codes are mapped to specific error classes:</p> <ul> <li>401: <code>HTTPUnauthorizedError</code></li> <li>403: <code>HTTPForbiddenError</code></li> <li>404: <code>HTTPNotFoundError</code></li> <li>408: <code>HTTPTimeoutError</code></li> <li>429: <code>HTTPTooManyRequestsError</code></li> <li>4xx: <code>HTTPClientError</code></li> <li>5xx: <code>HTTPServerError</code></li> </ul> <p>Network-related errors are mapped to the Transport error hierarchy:</p> <ul> <li><code>httpx.ConnectError</code>: <code>ConnectionError</code></li> <li><code>httpx.ConnectTimeout</code>: <code>ConnectionTimeoutError</code></li> <li><code>httpx.ReadTimeout</code>, <code>httpx.WriteTimeout</code>: <code>ConnectionTimeoutError</code></li> </ul>"},{"location":"http_transport/#http-features-support","title":"HTTP Features Support","text":"<p>The HTTP Transport Implementation supports a wide range of HTTP features:</p> <ul> <li>Query parameters: URL query parameters for GET requests</li> <li>Headers: Custom HTTP headers for all requests</li> <li>Form data: Form data for POST requests</li> <li>JSON: JSON data for request bodies</li> <li>Files: File uploads for multipart/form-data requests</li> <li>Raw content: Raw content for request bodies</li> <li>Streaming: Streaming responses for large payloads</li> </ul>"},{"location":"http_transport/#streaming-support","title":"Streaming Support","text":"<p>The HTTP Transport Implementation includes support for streaming responses, which is useful for handling large payloads efficiently. Streaming is implemented using the <code>stream_response</code> method, which returns an async iterator yielding chunks of the response as they are received.</p>"},{"location":"http_transport/#usage-examples","title":"Usage Examples","text":""},{"location":"http_transport/#basic-http-get-request","title":"Basic HTTP GET Request","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a GET request message\n    message = HttpMessage(\n        method=\"GET\",\n        url=\"/users\",\n        params={\"limit\": 10},\n    )\n\n    # Send the message\n    await t.send(message)\n\n    # Receive the response\n    async for response in t.receive():\n        data = response.get_payload()[\"data\"]\n        print(f\"Received {len(data)} users\")\n</code></pre>"},{"location":"http_transport/#http-post-with-json-data","title":"HTTP POST with JSON Data","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a POST request message with JSON data\n    message = HttpMessage(\n        method=\"POST\",\n        url=\"/users\",\n        headers={\"Content-Type\": \"application/json\"},\n        json_data={\n            \"name\": \"John Doe\",\n            \"email\": \"john.doe@example.com\",\n        },\n    )\n\n    # Send the message\n    await t.send(message)\n\n    # Receive the response\n    async for response in t.receive():\n        data = response.get_payload()[\"data\"]\n        print(f\"Created user with ID: {data['id']}\")\n</code></pre>"},{"location":"http_transport/#file-upload","title":"File Upload","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a POST request message with file upload\n    with open(\"avatar.png\", \"rb\") as f:\n        file_content = f.read()\n\n    message = HttpMessage(\n        method=\"POST\",\n        url=\"/users/1/avatar\",\n        files={\"avatar\": (\"avatar.png\", file_content, \"image/png\")},\n    )\n\n    # Send the message\n    await t.send(message)\n\n    # Receive the response\n    async for response in t.receive():\n        data = response.get_payload()[\"data\"]\n        print(f\"Uploaded avatar: {data['avatar_url']}\")\n</code></pre>"},{"location":"http_transport/#custom-headers-and-authentication","title":"Custom Headers and Authentication","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry with default headers for authentication\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n        default_headers={\n            \"Authorization\": \"Bearer YOUR_API_KEY\",\n            \"User-Agent\": \"Pynector/1.0\",\n        },\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a GET request message\n    message = HttpMessage(\n        method=\"GET\",\n        url=\"/protected-resource\",\n    )\n\n    # Send the message\n    await t.send(message)\n\n    # Receive the response\n    async for response in t.receive():\n        data = response.get_payload()[\"data\"]\n        print(f\"Received protected data: {data}\")\n</code></pre>"},{"location":"http_transport/#handling-errors","title":"Handling Errors","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import (\n    HttpMessage,\n    HTTPTransportFactory,\n    HTTPNotFoundError,\n    HTTPUnauthorizedError,\n    HTTPServerError,\n)\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\ntry:\n    async with transport as t:\n        # Create a GET request message\n        message = HttpMessage(\n            method=\"GET\",\n            url=\"/users/999\",  # Non-existent user\n        )\n\n        try:\n            # Send the message\n            await t.send(message)\n\n            # Receive the response\n            async for response in t.receive():\n                data = response.get_payload()[\"data\"]\n                print(f\"Received user: {data}\")\n        except HTTPNotFoundError:\n            print(\"User not found\")\n        except HTTPUnauthorizedError:\n            print(\"Authentication required\")\n        except HTTPServerError:\n            print(\"Server error occurred\")\nexcept Exception as e:\n    print(f\"Transport error: {e}\")\n</code></pre>"},{"location":"http_transport/#streaming-responses","title":"Streaming Responses","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a GET request message for a large file\n    message = HttpMessage(\n        method=\"GET\",\n        url=\"/large-file\",\n    )\n\n    # Stream the response\n    with open(\"large-file.dat\", \"wb\") as f:\n        async for chunk in t.stream_response(message):\n            f.write(chunk)\n            print(f\"Received {len(chunk)} bytes\")\n</code></pre>"},{"location":"http_transport/#configuring-retries","title":"Configuring Retries","text":"<pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.http import HttpMessage, HTTPTransportFactory\n\n# Set up registry with custom retry configuration\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"http\",\n    HTTPTransportFactory(\n        base_url=\"https://api.example.com\",\n        message_type=HttpMessage,\n        default_max_retries=5,\n        default_retry_backoff_factor=1.0,\n        default_retry_status_codes={429, 500, 502, 503, 504, 408},\n    ),\n)\n\n# Create a transport\ntransport = registry.create_transport(\"http\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Create a GET request message\n    message = HttpMessage(\n        method=\"GET\",\n        url=\"/flaky-endpoint\",  # Endpoint that might fail transiently\n    )\n\n    # Send the message (will retry up to 5 times with exponential backoff)\n    await t.send(message)\n\n    # Receive the response\n    async for response in t.receive():\n        data = response.get_payload()[\"data\"]\n        print(f\"Received data after retries: {data}\")\n</code></pre>"},{"location":"observability/","title":"Optional Observability","text":"<p>Pynector provides optional observability features through its telemetry module, including distributed tracing and structured logging. These features are designed to be optional dependencies, meaning Pynector will work correctly even if the observability libraries are not installed.</p>"},{"location":"observability/#key-features","title":"Key Features","text":"<ul> <li>Optional Dependencies: OpenTelemetry for tracing and structlog for logging   are optional dependencies.</li> <li>No-op Fallbacks: Graceful degradation when dependencies are not available.</li> <li>Context Propagation: Proper propagation of trace context across async   boundaries.</li> <li>Flexible Configuration: Configuration via environment variables or   programmatic API.</li> <li>Unified API: Consistent API regardless of whether dependencies are   available.</li> </ul>"},{"location":"observability/#components","title":"Components","text":"<p>The telemetry module consists of the following components:</p> <ol> <li> <p>Telemetry Facade: Provides a unified interface for tracing and logging    operations, abstracting away the details of the underlying implementations.</p> </li> <li> <p>No-op Implementations: Provide fallbacks when dependencies are not    available, ensuring that the library works correctly even without the    optional dependencies.</p> </li> <li> <p>Context Propagation: Ensures trace context is properly maintained across    async boundaries, allowing for accurate tracing of asynchronous operations.</p> </li> <li> <p>Configuration: Provides flexible configuration options via environment    variables and programmatic APIs.</p> </li> <li> <p>Dependency Detection: Detects whether optional dependencies are available    and sets appropriate flags.</p> </li> </ol>"},{"location":"observability/#installation","title":"Installation","text":"<p>To use the observability features, you need to install Pynector with the optional dependencies:</p> <pre><code># Install with all observability dependencies\npip install pynector[observability]\n\n# Or install individual dependencies\npip install pynector opentelemetry-api opentelemetry-sdk structlog\n</code></pre>"},{"location":"observability/#configuration","title":"Configuration","text":""},{"location":"observability/#environment-variables","title":"Environment Variables","text":"<p>The telemetry module can be configured using environment variables:</p> Variable Description Default <code>OTEL_SDK_DISABLED</code> Disable OpenTelemetry tracing <code>false</code> <code>OTEL_SERVICE_NAME</code> Service name for traces <code>\"unknown_service\"</code> <code>OTEL_RESOURCE_ATTRIBUTES</code> Comma-separated key-value pairs for resource attributes <code>{}</code> <code>OTEL_TRACES_EXPORTER</code> Comma-separated list of exporters to use (<code>otlp</code>, <code>console</code>, <code>zipkin</code>) <code>\"otlp\"</code> <code>OTEL_EXPORTER_OTLP_ENDPOINT</code> Endpoint for OTLP exporter OpenTelemetry default <code>OTEL_EXPORTER_ZIPKIN_ENDPOINT</code> Endpoint for Zipkin exporter Zipkin default"},{"location":"observability/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>You can also configure the telemetry module programmatically:</p> <pre><code>from pynector.telemetry import configure_telemetry\n\n# Configure with defaults\nconfigure_telemetry()\n\n# Or with custom settings\nconfigure_telemetry(\n    service_name=\"my-service\",\n    resource_attributes={\"deployment.environment\": \"production\"},\n    trace_enabled=True,\n    log_level=\"INFO\",\n    trace_exporters=[\"console\", \"otlp\"],\n)\n</code></pre>"},{"location":"observability/#usage","title":"Usage","text":""},{"location":"observability/#basic-usage","title":"Basic Usage","text":"<p>The simplest way to use the telemetry module is through the <code>get_telemetry</code> function:</p> <pre><code>from pynector.telemetry import get_telemetry\n\n# Get tracer and logger for a component\ntracer, logger = get_telemetry(\"my_component\")\n\n# Use the logger\nlogger.info(\"Operation started\", operation=\"process_data\")\n\n# Use the tracer\nwith tracer.start_as_current_span(\"process_data\") as span:\n    # Add attributes to the span\n    span.set_attribute(\"data.size\", 100)\n\n    # Do some work...\n\n    # Log within the span context (trace_id and span_id will be included)\n    logger.info(\"Processing data\", items=100)\n\n    # Record events\n    span.add_event(\"data_validated\", {\"valid_items\": 95})\n\n    # Handle errors\n    try:\n        # Do something that might fail\n        result = process_data()\n    except Exception as e:\n        # Record the exception and set error status\n        span.record_exception(e)\n        logger.error(\"Processing failed\", error=str(e))\n        raise\n</code></pre>"},{"location":"observability/#async-usage","title":"Async Usage","text":"<p>For async code, use the async span methods:</p> <pre><code>import asyncio\nfrom pynector.telemetry import get_telemetry\n\ntracer, logger = get_telemetry(\"my_async_component\")\n\nasync def process_item(item):\n    async with tracer.start_as_current_async_span(f\"process_{item}\") as span:\n        span.set_attribute(\"item.id\", item)\n        logger.info(f\"Processing item {item}\")\n        await asyncio.sleep(0.1)  # Simulate work\n        return item * 2\n\nasync def main():\n    async with tracer.start_as_current_async_span(\"main_process\") as span:\n        logger.info(\"Starting batch processing\")\n\n        # Process items in parallel while maintaining trace context\n        from pynector.telemetry.context import traced_gather\n\n        items = [1, 2, 3, 4, 5]\n        results = await traced_gather(\n            tracer,\n            [process_item(item) for item in items],\n            name=\"parallel_processing\"\n        )\n\n        logger.info(\"Batch processing complete\", results=results)\n        return results\n\n# Run the async code\nasyncio.run(main())\n</code></pre>"},{"location":"observability/#context-propagation-utilities","title":"Context Propagation Utilities","text":"<p>The telemetry module provides utilities for propagating context across async boundaries:</p> <pre><code>from pynector.telemetry import get_telemetry\nfrom pynector.telemetry.context import traced_async_operation, traced_gather, traced_task_group\n\ntracer, logger = get_telemetry(\"context_example\")\n\n# Use traced_async_operation for simple async operations\nasync def example_1():\n    async with traced_async_operation(tracer, \"my_operation\") as span:\n        # Do some work\n        span.set_attribute(\"example\", \"value\")\n        logger.info(\"Operation in progress\")\n\n# Use traced_gather for parallel operations\nasync def example_2():\n    async def task(i):\n        logger.info(f\"Task {i} started\")\n        return i * 2\n\n    results = await traced_gather(\n        tracer,\n        [task(i) for i in range(5)],\n        name=\"parallel_tasks\"\n    )\n    logger.info(\"All tasks completed\", results=results)\n\n# Use traced_task_group for more complex task management (requires anyio)\nasync def example_3():\n    task_group = await traced_task_group(tracer, \"task_group_example\")\n\n    async def worker(name):\n        logger.info(f\"Worker {name} started\")\n        # Trace context is propagated automatically\n\n    async with task_group:\n        task_group.start_soon(worker, \"A\")\n        task_group.start_soon(worker, \"B\")\n</code></pre>"},{"location":"observability/#behavior-when-dependencies-are-missing","title":"Behavior When Dependencies Are Missing","text":"<p>When the optional dependencies (OpenTelemetry and/or structlog) are not available, the telemetry module provides no-op implementations that maintain the same API but do nothing. This ensures that your code will work correctly even if the dependencies are not installed.</p>"},{"location":"observability/#tracing-without-opentelemetry","title":"Tracing Without OpenTelemetry","text":"<p>If OpenTelemetry is not available:</p> <ul> <li><code>TracingFacade</code> will use <code>NoOpSpan</code> implementations</li> <li>Spans will be created but will not record any data</li> <li>All span methods (<code>set_attribute</code>, <code>add_event</code>, etc.) will be available but   will do nothing</li> <li>Context propagation utilities will fall back to simpler implementations</li> </ul>"},{"location":"observability/#logging-without-structlog","title":"Logging Without structlog","text":"<p>If structlog is not available:</p> <ul> <li><code>LoggingFacade</code> will use <code>NoOpLogger</code> implementations</li> <li>Logger methods (<code>info</code>, <code>error</code>, etc.) will be available but will do nothing</li> </ul>"},{"location":"observability/#integration-with-other-systems","title":"Integration with Other Systems","text":""},{"location":"observability/#opentelemetry-exporters","title":"OpenTelemetry Exporters","text":"<p>The telemetry module supports multiple OpenTelemetry exporters:</p> <ul> <li>OTLP: Send traces to an OpenTelemetry collector</li> <li>Console: Print traces to the console (useful for debugging)</li> <li>Zipkin: Send traces to a Zipkin server</li> </ul> <p>You can configure which exporters to use via the <code>OTEL_TRACES_EXPORTER</code> environment variable or the <code>trace_exporters</code> parameter in <code>configure_telemetry()</code>.</p>"},{"location":"observability/#structured-logging","title":"Structured Logging","text":"<p>The telemetry module uses structlog for structured logging, which provides:</p> <ul> <li>JSON-formatted logs</li> <li>Automatic inclusion of trace context in logs</li> <li>Contextual information in all log entries</li> </ul>"},{"location":"observability/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Initialize Early: Call <code>configure_telemetry()</code> early in your application    startup.</p> </li> <li> <p>Use Descriptive Names: Use descriptive names for spans and log events.</p> </li> <li> <p>Add Context to Logs: Include relevant context in log entries using    keyword arguments.</p> </li> <li> <p>Propagate Context: Use the context propagation utilities for async code.</p> </li> <li> <p>Handle Errors: Record exceptions in spans and set appropriate status.</p> </li> <li> <p>Clean Up Resources: Use context managers (<code>with</code> and <code>async with</code>) to    ensure proper cleanup.</p> </li> <li> <p>Consider Performance: Be mindful of the performance impact of tracing and    logging in hot paths.</p> </li> </ol>"},{"location":"sdk_transport/","title":"SDK Transport Implementation","text":"<p>The SDK Transport Implementation is a concrete implementation of the Transport Abstraction Layer for interacting with AI model provider SDKs, such as OpenAI and Anthropic. It provides a unified interface for making requests to these services while conforming to the Transport Protocol defined in the Transport Abstraction Layer.</p>"},{"location":"sdk_transport/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Components</li> <li>SdkTransport</li> <li>SDK Adapters</li> <li>SdkTransportFactory</li> <li>SDK Error Hierarchy</li> <li>Features</li> <li>Adapter Pattern</li> <li>Error Translation</li> <li>Authentication Management</li> <li>Streaming Support</li> <li>Usage Examples</li> <li>Basic Usage</li> <li>OpenAI Example</li> <li>Anthropic Example</li> <li>Streaming Example</li> <li>Error Handling</li> <li>Custom Configuration</li> </ul>"},{"location":"sdk_transport/#overview","title":"Overview","text":"<p>The SDK Transport Implementation provides a complete solution for interacting with AI model provider SDKs within the Pynector framework. It follows the Transport Protocol defined in the Transport Abstraction Layer, ensuring compatibility with the rest of the framework while providing SDK-specific functionality.</p> <p>Key benefits of the SDK Transport Implementation include:</p> <ul> <li>Unified interface: Consistent interface for different AI model provider   SDKs</li> <li>Adapter pattern: Separation of transport logic from SDK-specific details</li> <li>Error translation: Mapping of SDK-specific errors to the Transport error   hierarchy</li> <li>Authentication management: Secure handling of API keys</li> <li>Streaming support: Unified streaming interface across different SDKs</li> <li>Configurability: Support for SDK-specific configuration options</li> </ul>"},{"location":"sdk_transport/#components","title":"Components","text":""},{"location":"sdk_transport/#sdktransport","title":"SdkTransport","text":"<p>The <code>SdkTransport</code> class is the core component of the SDK Transport Implementation. It implements the Transport Protocol and provides methods for connecting, disconnecting, sending, and receiving data from AI model provider SDKs.</p> <pre><code>class SdkTransport:\n    \"\"\"SDK transport implementation using OpenAI and Anthropic SDKs.\"\"\"\n\n    def __init__(\n        self,\n        sdk_type: str = \"openai\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        timeout: float = 60.0,\n        **kwargs: Any\n    ):\n        \"\"\"Initialize the transport with configuration options.\"\"\"\n        ...\n</code></pre> <p>The <code>SdkTransport</code> class provides the following methods:</p> <ul> <li><code>connect()</code>: Establishes the connection to the SDK</li> <li><code>disconnect()</code>: Closes the connection to the SDK</li> <li><code>send(data)</code>: Sends data to the SDK</li> <li><code>receive()</code>: Receives data from the SDK as an async iterator</li> </ul>"},{"location":"sdk_transport/#sdk-adapters","title":"SDK Adapters","text":"<p>The SDK adapters provide a consistent interface for interacting with different AI model provider SDKs. Each SDK has a corresponding adapter class that translates between the Transport Protocol and the SDK-specific API.</p>"},{"location":"sdk_transport/#sdkadapter","title":"SDKAdapter","text":"<p>The <code>SDKAdapter</code> is an abstract base class that defines the interface for all SDK-specific adapters:</p> <pre><code>class SDKAdapter(abc.ABC):\n    \"\"\"Base adapter class for SDK-specific implementations.\"\"\"\n\n    @abc.abstractmethod\n    async def complete(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; str:\n        \"\"\"Generate a completion for the given prompt.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    async def stream(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; AsyncIterator[bytes]:\n        \"\"\"Stream a completion for the given prompt.\"\"\"\n        pass\n</code></pre>"},{"location":"sdk_transport/#openaiadapter","title":"OpenAIAdapter","text":"<p>The <code>OpenAIAdapter</code> implements the <code>SDKAdapter</code> interface for the OpenAI SDK:</p> <pre><code>class OpenAIAdapter(SDKAdapter):\n    \"\"\"Adapter for the OpenAI SDK.\"\"\"\n\n    def __init__(self, client: openai.AsyncOpenAI):\n        \"\"\"Initialize the adapter with an OpenAI client.\"\"\"\n        self.client = client\n\n    async def complete(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; str:\n        \"\"\"Generate a completion using the OpenAI API.\"\"\"\n        ...\n\n    async def stream(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; AsyncIterator[bytes]:\n        \"\"\"Stream a completion using the OpenAI API.\"\"\"\n        ...\n</code></pre>"},{"location":"sdk_transport/#anthropicadapter","title":"AnthropicAdapter","text":"<p>The <code>AnthropicAdapter</code> implements the <code>SDKAdapter</code> interface for the Anthropic SDK:</p> <pre><code>class AnthropicAdapter(SDKAdapter):\n    \"\"\"Adapter for the Anthropic SDK.\"\"\"\n\n    def __init__(self, client: anthropic.AsyncAnthropic):\n        \"\"\"Initialize the adapter with an Anthropic client.\"\"\"\n        self.client = client\n\n    async def complete(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; str:\n        \"\"\"Generate a completion using the Anthropic API.\"\"\"\n        ...\n\n    async def stream(self, prompt: str, model: Optional[str] = None, **kwargs: Any) -&gt; AsyncIterator[bytes]:\n        \"\"\"Stream a completion using the Anthropic API.\"\"\"\n        ...\n</code></pre>"},{"location":"sdk_transport/#sdktransportfactory","title":"SdkTransportFactory","text":"<p>The <code>SdkTransportFactory</code> class implements the TransportFactory Protocol for creating SDK transport instances. It follows the Factory Method pattern, providing a way to create SDK transport instances with default configuration.</p> <pre><code>class SdkTransportFactory:\n    \"\"\"Factory for creating SDK transport instances.\"\"\"\n\n    def __init__(\n        self,\n        sdk_type: str = \"openai\",\n        api_key: Optional[str] = None,\n        base_url: Optional[str] = None,\n        timeout: float = 60.0,\n        **kwargs: Any\n    ):\n        \"\"\"Initialize the factory with default configuration options.\"\"\"\n        ...\n\n    def create_transport(self, **kwargs: Any) -&gt; SdkTransport:\n        \"\"\"Create a new SDK transport instance.\"\"\"\n        ...\n</code></pre>"},{"location":"sdk_transport/#sdk-error-hierarchy","title":"SDK Error Hierarchy","text":"<p>The SDK Transport Implementation defines a comprehensive error hierarchy for SDK-specific errors. This makes it easier to handle specific error conditions.</p> <pre><code>TransportError\n\u2514\u2500\u2500 TransportSpecificError\n    \u2514\u2500\u2500 SdkTransportError\n        \u251c\u2500\u2500 AuthenticationError\n        \u251c\u2500\u2500 RateLimitError\n        \u251c\u2500\u2500 InvalidRequestError\n        \u251c\u2500\u2500 ResourceNotFoundError\n        \u251c\u2500\u2500 PermissionError\n        \u2514\u2500\u2500 RequestTooLargeError\n</code></pre> <ul> <li>SdkTransportError: Base class for all SDK transport errors</li> <li>AuthenticationError: Error indicating an authentication failure</li> <li>RateLimitError: Error indicating a rate limit was exceeded</li> <li>InvalidRequestError: Error indicating an invalid request</li> <li>ResourceNotFoundError: Error indicating a resource was not found</li> <li>PermissionError: Error indicating a permission issue</li> <li>RequestTooLargeError: Error indicating a request was too large</li> </ul>"},{"location":"sdk_transport/#features","title":"Features","text":""},{"location":"sdk_transport/#adapter-pattern","title":"Adapter Pattern","text":"<p>The SDK Transport Implementation uses the adapter pattern to provide a consistent interface to different SDKs. This pattern allows the transport to work with different SDKs without changing its interface, making it easy to add support for new SDKs in the future.</p> <p>The adapter pattern is implemented through the <code>SDKAdapter</code> abstract base class and its concrete implementations (<code>OpenAIAdapter</code> and <code>AnthropicAdapter</code>). Each adapter translates between the Transport Protocol and the SDK-specific API.</p>"},{"location":"sdk_transport/#error-translation","title":"Error Translation","text":"<p>The SDK Transport Implementation includes comprehensive error translation, mapping SDK-specific errors to the Transport error hierarchy. This ensures a consistent error handling experience regardless of the underlying SDK.</p> <p>Error translation is implemented through the <code>_translate_error</code> method in the <code>SdkTransport</code> class. This method examines the error type and maps it to the appropriate Transport error class.</p> <p>For example, OpenAI's <code>AuthenticationError</code> is mapped to the Transport's <code>AuthenticationError</code>, and Anthropic's API status code 429 is mapped to the Transport's <code>RateLimitError</code>.</p>"},{"location":"sdk_transport/#authentication-management","title":"Authentication Management","text":"<p>Authentication in the SDK Transport Layer is handled through API keys. These can be provided directly or sourced from environment variables.</p> <p>For OpenAI, the priority for authentication is:</p> <ol> <li>API key provided to the constructor</li> <li><code>OPENAI_API_KEY</code> environment variable</li> </ol> <p>For Anthropic, the priority for authentication is:</p> <ol> <li>API key provided to the constructor</li> <li><code>ANTHROPIC_API_KEY</code> environment variable</li> </ol> <p>This approach provides flexibility while maintaining security, as API keys are never logged or exposed in error messages.</p>"},{"location":"sdk_transport/#streaming-support","title":"Streaming Support","text":"<p>The SDK Transport Implementation provides a unified streaming interface across different SDKs. This allows clients to consume streaming responses consistently, regardless of the underlying SDK.</p> <p>Streaming is implemented through the <code>stream</code> method in the SDK adapters and exposed through the <code>receive</code> method in the <code>SdkTransport</code> class. This method returns an async iterator that yields chunks of the response as they are received.</p>"},{"location":"sdk_transport/#usage-examples","title":"Usage Examples","text":""},{"location":"sdk_transport/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic example of how to use the SDK Transport Implementation:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"openai\", SdkTransportFactory(sdk_type=\"openai\"))\n\n# Create a transport\ntransport = registry.create_transport(\"openai\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a prompt\n    await t.send(b\"Tell me a joke about programming\")\n\n    # Receive the response\n    async for chunk in t.receive():\n        print(chunk.decode(\"utf-8\"), end=\"\")\n</code></pre>"},{"location":"sdk_transport/#openai-example","title":"OpenAI Example","text":"<p>Here's an example of how to use the SDK Transport Implementation with OpenAI:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"openai\",\n    SdkTransportFactory(\n        sdk_type=\"openai\",\n        model=\"gpt-4o\",\n        temperature=0.7,\n        max_tokens=1000\n    )\n)\n\n# Create a transport\ntransport = registry.create_transport(\"openai\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a prompt\n    await t.send(b\"Explain quantum computing in simple terms\")\n\n    # Receive the response\n    async for chunk in t.receive():\n        print(chunk.decode(\"utf-8\"), end=\"\")\n</code></pre>"},{"location":"sdk_transport/#anthropic-example","title":"Anthropic Example","text":"<p>Here's an example of how to use the SDK Transport Implementation with Anthropic:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"anthropic\",\n    SdkTransportFactory(\n        sdk_type=\"anthropic\",\n        model=\"claude-3-opus-20240229\",\n        temperature=0.5,\n        max_tokens=2000\n    )\n)\n\n# Create a transport\ntransport = registry.create_transport(\"anthropic\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a prompt\n    await t.send(b\"Write a short story about a robot that learns to love\")\n\n    # Receive the response\n    async for chunk in t.receive():\n        print(chunk.decode(\"utf-8\"), end=\"\")\n</code></pre>"},{"location":"sdk_transport/#streaming-example","title":"Streaming Example","text":"<p>Here's an example of how to use the streaming interface:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"openai\", SdkTransportFactory(sdk_type=\"openai\"))\n\n# Create a transport\ntransport = registry.create_transport(\"openai\")\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a prompt\n    await t.send(b\"Generate a long response about the history of artificial intelligence\")\n\n    # Stream the response\n    async for chunk in t.receive():\n        # Process each chunk as it arrives\n        print(chunk.decode(\"utf-8\"), end=\"\", flush=True)\n</code></pre>"},{"location":"sdk_transport/#error-handling","title":"Error Handling","text":"<p>Here's an example of how to handle errors:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\nfrom pynector.transport.sdk.errors import (\n    SdkTransportError,\n    AuthenticationError,\n    RateLimitError,\n    InvalidRequestError,\n    ResourceNotFoundError,\n    PermissionError,\n    RequestTooLargeError\n)\nfrom pynector.transport.errors import ConnectionError, ConnectionTimeoutError\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"openai\", SdkTransportFactory(sdk_type=\"openai\"))\n\n# Create a transport\ntransport = registry.create_transport(\"openai\")\n\ntry:\n    async with transport as t:\n        try:\n            # Send a prompt\n            await t.send(b\"Tell me a joke about programming\")\n\n            # Receive the response\n            async for chunk in t.receive():\n                print(chunk.decode(\"utf-8\"), end=\"\")\n        except AuthenticationError:\n            print(\"Authentication failed. Check your API key.\")\n        except RateLimitError:\n            print(\"Rate limit exceeded. Try again later.\")\n        except InvalidRequestError as e:\n            print(f\"Invalid request: {e}\")\n        except ResourceNotFoundError:\n            print(\"Resource not found. Check your model name.\")\n        except SdkTransportError as e:\n            print(f\"SDK error: {e}\")\nexcept ConnectionError as e:\n    print(f\"Connection error: {e}\")\nexcept ConnectionTimeoutError as e:\n    print(f\"Connection timeout: {e}\")\n</code></pre>"},{"location":"sdk_transport/#custom-configuration","title":"Custom Configuration","text":"<p>Here's an example of how to use custom configuration options:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.sdk.factory import SdkTransportFactory\n\n# Set up registry with custom configuration\nregistry = TransportFactoryRegistry()\nregistry.register(\n    \"openai\",\n    SdkTransportFactory(\n        sdk_type=\"openai\",\n        api_key=\"your-api-key\",  # Directly provide API key\n        base_url=\"https://custom-openai-endpoint.com/v1\",  # Custom endpoint\n        timeout=30.0,  # Custom timeout\n        model=\"gpt-4o\",  # Default model\n        temperature=0.7,  # Model-specific parameter\n        max_tokens=1000,  # Model-specific parameter\n        organization=\"your-organization-id\"  # OpenAI-specific parameter\n    )\n)\n\n# Create a transport with additional configuration\ntransport = registry.create_transport(\n    \"openai\",\n    model=\"gpt-3.5-turbo\",  # Override the default model\n    temperature=0.9  # Override the default temperature\n)\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a prompt\n    await t.send(b\"Tell me a joke about programming\")\n\n    # Receive the response\n    async for chunk in t.receive():\n        print(chunk.decode(\"utf-8\"), end=\"\")\n</code></pre>"},{"location":"transport/","title":"Transport Abstraction Layer","text":"<p>The Transport Abstraction Layer is a core component of Pynector that provides a flexible and maintainable interface for network communication. It follows the sans-I/O pattern, which separates I/O concerns from protocol logic, making it easier to test, maintain, and extend.</p>"},{"location":"transport/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Design Philosophy</li> <li>Components</li> <li>Transport Protocol</li> <li>Message Protocol</li> <li>Error Hierarchy</li> <li>Message Implementations</li> <li>Transport Factory</li> <li>Transport Factory Registry</li> <li>Transport Implementations</li> <li>HTTP Transport</li> <li>Usage Examples</li> <li>Basic Usage</li> <li>Error Handling</li> <li>Using Multiple Transports</li> <li>Implementing Custom Transports</li> <li>Implementing Custom Message Formats</li> </ul>"},{"location":"transport/#design-philosophy","title":"Design Philosophy","text":"<p>The Transport Abstraction Layer is designed with the following principles in mind:</p>"},{"location":"transport/#sans-io-pattern","title":"Sans-I/O Pattern","text":"<p>The sans-I/O pattern separates I/O concerns from protocol logic. This means that the protocol implementation doesn't directly perform I/O operations, but instead defines how to interpret and generate data. This separation has several benefits:</p> <ul> <li>Testability: Protocol logic can be tested without actual I/O, making tests   faster and more reliable.</li> <li>Flexibility: The same protocol implementation can be used with different   I/O mechanisms (synchronous, asynchronous, etc.).</li> <li>Maintainability: Changes to I/O mechanisms don't affect protocol logic,   and vice versa.</li> </ul>"},{"location":"transport/#protocol-based-design","title":"Protocol-Based Design","text":"<p>The Transport Abstraction Layer uses Python's Protocol classes (from the <code>typing</code> module) to define interfaces. This enables static type checking and makes it clear what methods a class must implement to satisfy the interface.</p>"},{"location":"transport/#async-context-management","title":"Async Context Management","text":"<p>The Transport Abstraction Layer uses async context managers for resource handling. This ensures that resources are properly acquired and released, even in the presence of exceptions.</p>"},{"location":"transport/#components","title":"Components","text":""},{"location":"transport/#transport-protocol","title":"Transport Protocol","text":"<p>The Transport Protocol defines the interface for all transport implementations. It includes methods for connecting, disconnecting, sending, and receiving messages.</p> <pre><code>from collections.abc import AsyncIterator\nfrom typing import Protocol, TypeVar\n\nT = TypeVar(\"T\")\n\nclass Transport(Protocol, Generic[T]):\n    \"\"\"Protocol defining the interface for transport implementations.\"\"\"\n\n    async def connect(self) -&gt; None:\n        \"\"\"Establish the connection to the remote endpoint.\"\"\"\n        ...\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Close the connection to the remote endpoint.\"\"\"\n        ...\n\n    async def send(self, message: T) -&gt; None:\n        \"\"\"Send a message over the transport.\"\"\"\n        ...\n\n    async def receive(self) -&gt; AsyncIterator[T]:\n        \"\"\"Receive messages from the transport.\"\"\"\n        ...\n\n    async def __aenter__(self) -&gt; \"Transport[T]\":\n        \"\"\"Enter the async context, establishing the connection.\"\"\"\n        ...\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Exit the async context, closing the connection.\"\"\"\n        ...\n</code></pre>"},{"location":"transport/#message-protocol","title":"Message Protocol","text":"<p>The Message Protocol defines the interface for message serialization and deserialization. It includes methods for converting messages to and from bytes, as well as accessing message headers and payload.</p> <pre><code>from typing import Any, Protocol, TypeVar\n\nM = TypeVar(\"M\", bound=\"Message\")\n\nclass Message(Protocol):\n    \"\"\"Protocol defining the interface for message serialization/deserialization.\"\"\"\n\n    def serialize(self) -&gt; bytes:\n        \"\"\"Convert the message to bytes for transmission.\"\"\"\n        ...\n\n    @classmethod\n    def deserialize(cls: type[M], data: bytes) -&gt; M:\n        \"\"\"Create a message from received bytes.\"\"\"\n        ...\n\n    def get_headers(self) -&gt; dict[str, Any]:\n        \"\"\"Get the message headers.\"\"\"\n        ...\n\n    def get_payload(self) -&gt; Any:\n        \"\"\"Get the message payload.\"\"\"\n        ...\n</code></pre>"},{"location":"transport/#error-hierarchy","title":"Error Hierarchy","text":"<p>The Transport Abstraction Layer defines a comprehensive error hierarchy for transport-related errors. This makes it easier to handle specific error conditions.</p> <pre><code>TransportError\n\u251c\u2500\u2500 ConnectionError\n\u2502   \u251c\u2500\u2500 ConnectionTimeoutError\n\u2502   \u2514\u2500\u2500 ConnectionRefusedError\n\u251c\u2500\u2500 MessageError\n\u2502   \u251c\u2500\u2500 SerializationError\n\u2502   \u2514\u2500\u2500 DeserializationError\n\u2514\u2500\u2500 TransportSpecificError\n</code></pre> <ul> <li>TransportError: Base class for all transport-related errors.</li> <li>ConnectionError: Error indicating a connection problem.</li> <li>ConnectionTimeoutError: Error indicating a connection timeout.</li> <li>ConnectionRefusedError: Error indicating a connection was refused.</li> <li>MessageError: Error related to message handling.</li> <li>SerializationError: Error during message serialization.</li> <li>DeserializationError: Error during message deserialization.</li> <li>TransportSpecificError: Base class for transport-specific errors.</li> </ul>"},{"location":"transport/#message-implementations","title":"Message Implementations","text":"<p>The Transport Abstraction Layer includes two message implementations:</p>"},{"location":"transport/#jsonmessage","title":"JsonMessage","text":"<p>The <code>JsonMessage</code> class implements the Message protocol with JSON serialization. It's suitable for text-based protocols and human-readable messages.</p> <pre><code>from typing import Any\n\nfrom pynector.transport.errors import DeserializationError, SerializationError\n\nclass JsonMessage:\n    \"\"\"JSON-serialized message implementation.\"\"\"\n\n    content_type: ClassVar[str] = \"application/json\"\n\n    def __init__(self, headers: dict[str, Any], payload: Any):\n        \"\"\"Initialize a new JSON message.\"\"\"\n        self.headers = headers\n        self.payload = payload\n\n    def serialize(self) -&gt; bytes:\n        \"\"\"Convert the message to bytes for transmission.\"\"\"\n        data = {\"headers\": self.headers, \"payload\": self.payload}\n        try:\n            return json.dumps(data).encode(\"utf-8\")\n        except (TypeError, ValueError) as e:\n            raise SerializationError(f\"Failed to serialize JSON message: {e}\")\n\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; \"JsonMessage\":\n        \"\"\"Create a message from received bytes.\"\"\"\n        try:\n            parsed = json.loads(data.decode(\"utf-8\"))\n            return cls(\n                headers=parsed.get(\"headers\", {}),\n                payload=parsed.get(\"payload\", None)\n            )\n        except json.JSONDecodeError as e:\n            raise DeserializationError(f\"Invalid JSON data: {e}\")\n        except UnicodeDecodeError as e:\n            raise DeserializationError(f\"Invalid UTF-8 encoding: {e}\")\n\n    def get_headers(self) -&gt; dict[str, Any]:\n        \"\"\"Get the message headers.\"\"\"\n        return self.headers\n\n    def get_payload(self) -&gt; Any:\n        \"\"\"Get the message payload.\"\"\"\n        return self.payload\n</code></pre>"},{"location":"transport/#binarymessage","title":"BinaryMessage","text":"<p>The <code>BinaryMessage</code> class implements the Message protocol with binary serialization. It's suitable for binary protocols and efficient transmission.</p> <pre><code>from typing import Any\n\nfrom pynector.transport.errors import DeserializationError, SerializationError\n\nclass BinaryMessage:\n    \"\"\"Binary message implementation.\"\"\"\n\n    content_type: ClassVar[str] = \"application/octet-stream\"\n\n    def __init__(self, headers: dict[str, Any], payload: bytes):\n        \"\"\"Initialize a new binary message.\"\"\"\n        self.headers = headers\n        self.payload = payload\n\n    def serialize(self) -&gt; bytes:\n        \"\"\"Convert the message to bytes for transmission.\"\"\"\n        # Simple format: 4-byte header length + header JSON + payload\n        try:\n            header_json = json.dumps(self.headers).encode(\"utf-8\")\n            header_len = len(header_json)\n            return header_len.to_bytes(4, byteorder=\"big\") + header_json + self.payload\n        except (TypeError, ValueError) as e:\n            raise SerializationError(f\"Failed to serialize binary message: {e}\")\n\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; \"BinaryMessage\":\n        \"\"\"Create a message from received bytes.\"\"\"\n        try:\n            if len(data) &lt; 4:\n                raise DeserializationError(\"Message too short\")\n\n            header_len = int.from_bytes(data[:4], byteorder=\"big\")\n            if len(data) &lt; 4 + header_len:\n                raise DeserializationError(\"Message truncated\")\n\n            header_json = data[4 : 4 + header_len]\n            headers = json.loads(header_json.decode(\"utf-8\"))\n            payload = data[4 + header_len :]\n\n            return cls(headers=headers, payload=payload)\n        except (json.JSONDecodeError, UnicodeDecodeError) as e:\n            raise DeserializationError(f\"Invalid binary message format: {e}\")\n        except (ValueError, OverflowError) as e:\n            raise DeserializationError(f\"Invalid binary message structure: {e}\")\n\n    def get_headers(self) -&gt; dict[str, Any]:\n        \"\"\"Get the message headers.\"\"\"\n        return self.headers\n\n    def get_payload(self) -&gt; bytes:\n        \"\"\"Get the message payload.\"\"\"\n        return self.payload\n</code></pre>"},{"location":"transport/#transport-factory","title":"Transport Factory","text":"<p>The Transport Factory defines the interface for creating transport instances. It follows the Factory Method pattern, which provides a way to create objects without specifying the exact class of object that will be created.</p> <pre><code>from typing import Any, Protocol, TypeVar\n\nT = TypeVar(\"T\")\n\nclass TransportFactory(Protocol, Generic[T]):\n    \"\"\"Protocol defining the interface for transport factories.\"\"\"\n\n    def create_transport(self, **kwargs: Any) -&gt; T:\n        \"\"\"Create a new transport instance.\"\"\"\n        ...\n</code></pre>"},{"location":"transport/#transport-factory-registry","title":"Transport Factory Registry","text":"<p>The Transport Factory Registry provides a registry for transport factories. It allows for dynamic registration and lookup of transport factories.</p> <pre><code>from typing import Any\n\nfrom pynector.transport.factory import TransportFactory\nfrom pynector.transport.protocol import Transport\n\nclass TransportFactoryRegistry:\n    \"\"\"Registry for transport factories.\"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize a new transport factory registry.\"\"\"\n        self._factories = {}\n\n    def register(self, name: str, factory: TransportFactory) -&gt; None:\n        \"\"\"Register a transport factory.\"\"\"\n        self._factories[name] = factory\n\n    def get(self, name: str) -&gt; TransportFactory:\n        \"\"\"Get a transport factory by name.\"\"\"\n        return self._factories[name]\n\n    def create_transport(self, name: str, **kwargs: Any) -&gt; Transport:\n        \"\"\"Create a transport using a registered factory.\"\"\"\n        factory = self.get(name)\n        return factory.create_transport(**kwargs)\n</code></pre>"},{"location":"transport/#transport-implementations","title":"Transport Implementations","text":"<p>The Transport Abstraction Layer includes implementations for common transport protocols.</p>"},{"location":"transport/#http-transport","title":"HTTP Transport","text":"<p>The HTTP Transport Implementation provides a complete solution for HTTP communication within the Pynector framework. It is built on the <code>httpx</code> library and provides a robust, feature-rich HTTP client with support for modern HTTP features.</p> <p>Key features of the HTTP Transport Implementation include:</p> <ul> <li>Async-first design: Built on <code>httpx.AsyncClient</code> for efficient   asynchronous HTTP communication</li> <li>Connection pooling: Reuses connections for improved performance</li> <li>Comprehensive error handling: Maps HTTP errors to the Transport error   hierarchy</li> <li>Retry mechanism: Automatically retries failed requests with exponential   backoff</li> <li>Support for modern HTTP features: Includes query parameters, headers, form   data, JSON, and file uploads</li> <li>Streaming support: Efficiently handles large responses with streaming</li> </ul> <p>For detailed documentation on the HTTP Transport Implementation, see HTTP Transport Documentation.</p>"},{"location":"transport/#usage-examples","title":"Usage Examples","text":""},{"location":"transport/#basic-usage","title":"Basic Usage","text":"<p>Here's a basic example of how to use the Transport Abstraction Layer:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.message import JsonMessage\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"my_transport\", MyTransportFactory())\n\n# Create a transport\ntransport = registry.create_transport(\"my_transport\", host=\"example.com\", port=8080)\n\n# Use the transport with async context manager\nasync with transport as t:\n    # Send a message\n    await t.send(JsonMessage({\"content-type\": \"application/json\"}, {\"data\": \"Hello, World!\"}))\n\n    # Receive messages\n    async for message in t.receive():\n        print(f\"Received: {message.get_payload()}\")\n</code></pre>"},{"location":"transport/#error-handling","title":"Error Handling","text":"<p>Here's an example of how to handle errors:</p> <pre><code>from pynector.transport import (\n    TransportFactoryRegistry,\n    ConnectionError,\n    ConnectionTimeoutError,\n    ConnectionRefusedError,\n    MessageError,\n    SerializationError,\n    DeserializationError,\n)\nfrom pynector.transport.message import JsonMessage\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"my_transport\", MyTransportFactory())\n\n# Create a transport\ntransport = registry.create_transport(\"my_transport\", host=\"example.com\", port=8080)\n\ntry:\n    async with transport as t:\n        try:\n            # Send a message\n            await t.send(JsonMessage({\"content-type\": \"application/json\"}, {\"data\": \"Hello, World!\"}))\n        except ConnectionError as e:\n            print(f\"Connection error while sending: {e}\")\n        except SerializationError as e:\n            print(f\"Serialization error: {e}\")\n\n        try:\n            # Receive messages\n            async for message in t.receive():\n                print(f\"Received: {message.get_payload()}\")\n        except ConnectionError as e:\n            print(f\"Connection error while receiving: {e}\")\n        except DeserializationError as e:\n            print(f\"Deserialization error: {e}\")\nexcept ConnectionTimeoutError as e:\n    print(f\"Connection timeout: {e}\")\nexcept ConnectionRefusedError as e:\n    print(f\"Connection refused: {e}\")\nexcept ConnectionError as e:\n    print(f\"Other connection error: {e}\")\n</code></pre>"},{"location":"transport/#using-multiple-transports","title":"Using Multiple Transports","text":"<p>Here's an example of how to use multiple transports:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\nfrom pynector.transport.message import JsonMessage, BinaryMessage\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"json_transport\", JsonTransportFactory())\nregistry.register(\"binary_transport\", BinaryTransportFactory())\n\n# Create transports\njson_transport = registry.create_transport(\"json_transport\", host=\"example.com\", port=8080)\nbinary_transport = registry.create_transport(\"binary_transport\", host=\"example.org\", port=9090)\n\n# Use both transports\nasync with json_transport as jt, binary_transport as bt:\n    # Send messages\n    await jt.send(JsonMessage({\"content-type\": \"application/json\"}, {\"data\": \"Hello, JSON!\"}))\n    await bt.send(BinaryMessage({\"content-type\": \"application/octet-stream\"}, b\"Hello, Binary!\"))\n\n    # Receive messages from both\n    json_messages = [msg async for msg in jt.receive()]\n    binary_messages = [msg async for msg in bt.receive()]\n\n    # Process messages\n    for msg in json_messages:\n        print(f\"JSON message: {msg.get_payload()}\")\n\n    for msg in binary_messages:\n        print(f\"Binary message: {msg.get_payload()}\")\n</code></pre>"},{"location":"transport/#implementing-custom-transports","title":"Implementing Custom Transports","text":"<p>To implement a custom transport, you need to create a class that satisfies the Transport protocol:</p> <pre><code>from collections.abc import AsyncIterator\nfrom typing import Any\n\nfrom pynector.transport import ConnectionError, TransportError\nfrom pynector.transport.message import JsonMessage\n\nclass MyJsonTransport:\n    \"\"\"Custom JSON transport implementation.\"\"\"\n\n    def __init__(self, host: str, port: int):\n        \"\"\"Initialize a new transport.\"\"\"\n        self.host = host\n        self.port = port\n        self.connected = False\n        self.connection = None\n\n    async def connect(self) -&gt; None:\n        \"\"\"Establish the connection to the remote endpoint.\"\"\"\n        try:\n            # Implement connection logic here\n            self.connection = await some_library.connect(self.host, self.port)\n            self.connected = True\n        except some_library.ConnectionError as e:\n            raise ConnectionError(f\"Failed to connect to {self.host}:{self.port}: {e}\")\n        except some_library.TimeoutError as e:\n            raise ConnectionTimeoutError(f\"Connection to {self.host}:{self.port} timed out: {e}\")\n\n    async def disconnect(self) -&gt; None:\n        \"\"\"Close the connection to the remote endpoint.\"\"\"\n        if self.connected and self.connection:\n            try:\n                await self.connection.close()\n            finally:\n                self.connected = False\n                self.connection = None\n\n    async def send(self, message: JsonMessage) -&gt; None:\n        \"\"\"Send a message over the transport.\"\"\"\n        if not self.connected or not self.connection:\n            raise ConnectionError(\"Not connected\")\n\n        try:\n            data = message.serialize()\n            await self.connection.send(data)\n        except some_library.ConnectionError as e:\n            self.connected = False\n            self.connection = None\n            raise ConnectionError(f\"Connection lost while sending: {e}\")\n        except Exception as e:\n            raise TransportError(f\"Error sending message: {e}\")\n\n    async def receive(self) -&gt; AsyncIterator[JsonMessage]:\n        \"\"\"Receive messages from the transport.\"\"\"\n        if not self.connected or not self.connection:\n            raise ConnectionError(\"Not connected\")\n\n        try:\n            while self.connected:\n                data = await self.connection.receive()\n                if not data:  # Connection closed\n                    self.connected = False\n                    self.connection = None\n                    break\n\n                yield JsonMessage.deserialize(data)\n        except some_library.ConnectionError as e:\n            self.connected = False\n            self.connection = None\n            raise ConnectionError(f\"Connection lost while receiving: {e}\")\n        except Exception as e:\n            raise TransportError(f\"Error receiving message: {e}\")\n\n    async def __aenter__(self) -&gt; \"MyJsonTransport\":\n        \"\"\"Enter the async context, establishing the connection.\"\"\"\n        await self.connect()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb) -&gt; None:\n        \"\"\"Exit the async context, closing the connection.\"\"\"\n        await self.disconnect()\n</code></pre> <p>Then, create a factory for your transport:</p> <pre><code>from typing import Any\n\nfrom pynector.transport.factory import TransportFactory\n\nclass MyJsonTransportFactory:\n    \"\"\"Factory for creating MyJsonTransport instances.\"\"\"\n\n    def create_transport(self, **kwargs: Any) -&gt; MyJsonTransport:\n        \"\"\"Create a new transport instance.\"\"\"\n        host = kwargs.get(\"host\")\n        port = kwargs.get(\"port\")\n\n        if not host:\n            raise ValueError(\"Host is required\")\n        if not port:\n            raise ValueError(\"Port is required\")\n\n        return MyJsonTransport(host=host, port=port)\n</code></pre> <p>Finally, register your factory with the registry:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"my_json\", MyJsonTransportFactory())\n\n# Create a transport\ntransport = registry.create_transport(\"my_json\", host=\"example.com\", port=8080)\n</code></pre>"},{"location":"transport/#implementing-custom-message-formats","title":"Implementing Custom Message Formats","text":"<p>To implement a custom message format, you need to create a class that satisfies the Message protocol:</p> <pre><code>from typing import Any, ClassVar\n\nfrom pynector.transport.errors import DeserializationError, SerializationError\n\nclass MyCustomMessage:\n    \"\"\"Custom message implementation.\"\"\"\n\n    content_type: ClassVar[str] = \"application/x-custom\"\n\n    def __init__(self, headers: dict[str, Any], payload: Any):\n        \"\"\"Initialize a new custom message.\"\"\"\n        self.headers = headers\n        self.payload = payload\n\n    def serialize(self) -&gt; bytes:\n        \"\"\"Convert the message to bytes for transmission.\"\"\"\n        # Implement your custom serialization logic here\n        try:\n            # Example: Simple format with header length + header + payload\n            header_bytes = some_library.serialize(self.headers)\n            payload_bytes = some_library.serialize(self.payload)\n            header_len = len(header_bytes)\n            return header_len.to_bytes(4, byteorder=\"big\") + header_bytes + payload_bytes\n        except Exception as e:\n            raise SerializationError(f\"Failed to serialize custom message: {e}\")\n\n    @classmethod\n    def deserialize(cls, data: bytes) -&gt; \"MyCustomMessage\":\n        \"\"\"Create a message from received bytes.\"\"\"\n        try:\n            # Implement your custom deserialization logic here\n            if len(data) &lt; 4:\n                raise DeserializationError(\"Message too short\")\n\n            header_len = int.from_bytes(data[:4], byteorder=\"big\")\n            if len(data) &lt; 4 + header_len:\n                raise DeserializationError(\"Message truncated\")\n\n            header_bytes = data[4 : 4 + header_len]\n            payload_bytes = data[4 + header_len :]\n\n            headers = some_library.deserialize(header_bytes)\n            payload = some_library.deserialize(payload_bytes)\n\n            return cls(headers=headers, payload=payload)\n        except Exception as e:\n            raise DeserializationError(f\"Failed to deserialize custom message: {e}\")\n\n    def get_headers(self) -&gt; dict[str, Any]:\n        \"\"\"Get the message headers.\"\"\"\n        return self.headers\n\n    def get_payload(self) -&gt; Any:\n        \"\"\"Get the message payload.\"\"\"\n        return self.payload\n</code></pre> <p>Then, you can use your custom message format with any transport that supports it:</p> <pre><code>from pynector.transport import TransportFactoryRegistry\n\n# Set up registry\nregistry = TransportFactoryRegistry()\nregistry.register(\"my_transport\", MyTransportFactory())\n\n# Create a transport\ntransport = registry.create_transport(\"my_transport\", host=\"example.com\", port=8080)\n\n# Use the transport with your custom message format\nasync with transport as t:\n    # Send a message\n    await t.send(MyCustomMessage({\"content-type\": \"application/x-custom\"}, {\"data\": \"Hello, Custom!\"}))\n\n    # Receive messages\n    async for message in t.receive():\n        print(f\"Received: {message.get_payload()}\")\n</code></pre>"},{"location":"development/architecture/","title":"Pynector Architecture","text":"<p>This document provides an overview of Pynector's architecture, design patterns, and key components.</p>"},{"location":"development/architecture/#core-design-principles","title":"Core Design Principles","text":"<p>Pynector is built on several core design principles:</p> <ol> <li>Modularity: Components are decoupled and can be used independently.</li> <li>Extensibility: Transport system is pluggable and customizable.</li> <li>Reliability: Built-in error handling, retries, and timeout mechanisms.</li> <li>Observability: Integrated logging and tracing for monitoring and    debugging.</li> <li>Type Safety: Comprehensive type hints throughout the codebase.</li> </ol>"},{"location":"development/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>+----------------+\n|     Client     |\n+----------------+\n        |\n+----------------+\n|    Transport    |\n+----------------+\n        |\n+----------------+     +----------------+\n|   Concurrency  |     |  Telemetry     |\n+----------------+     +----------------+\n</code></pre>"},{"location":"development/architecture/#major-components","title":"Major Components","text":""},{"location":"development/architecture/#client","title":"Client","text":"<p>The <code>Client</code> class provides a high-level interface for making requests. It handles:</p> <ul> <li>Configuration and initialization of transport layers</li> <li>Request preparation and sending</li> <li>Response parsing and error handling</li> <li>Resource management (connections, etc.)</li> </ul>"},{"location":"development/architecture/#transport-layer","title":"Transport Layer","text":"<p>The transport layer is responsible for the actual communication with external services:</p> <ul> <li>HTTP Transport: Uses <code>httpx</code> to make HTTP/HTTPS requests.</li> <li>SDK Transport: Adapts existing Python SDKs into the Pynector interface.</li> <li>Custom Transports: Can be implemented by extending the base <code>Transport</code>   protocol.</li> </ul>"},{"location":"development/architecture/#concurrency-module","title":"Concurrency Module","text":"<p>The concurrency module provides tools for handling asynchronous operations:</p> <ul> <li>Task Management: Creation, cancellation, and tracking of async tasks.</li> <li>Concurrency Patterns: Common async patterns like <code>gather</code>, <code>race</code>, etc.</li> <li>Cancellation Support: Proper cancellation handling for async operations.</li> </ul>"},{"location":"development/architecture/#telemetry-module","title":"Telemetry Module","text":"<p>The telemetry module handles observability concerns:</p> <ul> <li>Logging: Structured logging with context propagation.</li> <li>Tracing: Distributed tracing support with OpenTelemetry.</li> <li>Metrics: Basic performance metrics collection.</li> </ul>"},{"location":"development/architecture/#data-flow","title":"Data Flow","text":"<p>A typical request flow through Pynector:</p> <ol> <li>The user creates a <code>Client</code> with a specific configuration.</li> <li>The user calls a method like <code>client.get()</code> or <code>client.post()</code>.</li> <li>The client prepares the request and passes it to the appropriate transport.</li> <li>The transport executes the request and returns a response.</li> <li>The client wraps the response and returns it to the user.</li> <li>Throughout this process, telemetry data is collected and can be exported.</li> </ol>"},{"location":"development/architecture/#error-handling","title":"Error Handling","text":"<p>Pynector uses a hierarchical error system:</p> <ul> <li>Base <code>PynectorError</code> for all errors</li> <li><code>RequestError</code> for connection issues, timeouts, etc.</li> <li><code>ResponseError</code> for server-side errors</li> <li>Transport-specific errors (e.g., <code>HTTPError</code>, <code>SDKError</code>)</li> </ul>"},{"location":"development/architecture/#extension-points","title":"Extension Points","text":"<p>Pynector can be extended through several mechanisms:</p> <ul> <li>Custom Transports: Implement the <code>Transport</code> protocol</li> <li>Custom Adapters: Create SDK adapters for specific services</li> <li>Middleware: Add request/response processing logic</li> <li>Telemetry Integrations: Connect to custom observability systems</li> </ul>"},{"location":"development/architecture/#configuration-system","title":"Configuration System","text":"<p>Pynector uses a central configuration system that allows:</p> <ul> <li>Global defaults</li> <li>Per-client configuration</li> <li>Per-request overrides</li> <li>Environment variable support</li> </ul>"},{"location":"development/code-of-conduct/","title":"Code of Conduct","text":""},{"location":"development/code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"development/code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"development/code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Project maintainers are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"development/code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"development/code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [INSERT CONTACT METHOD]. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"development/code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.</p>"},{"location":"development/contributing/","title":"Contributing to Pynector","text":"<p>We love your input! We want to make contributing to Pynector as easy and transparent as possible, whether it's:</p> <ul> <li>Reporting a bug</li> <li>Discussing the current state of the code</li> <li>Submitting a fix</li> <li>Proposing new features</li> <li>Becoming a maintainer</li> </ul>"},{"location":"development/contributing/#development-process","title":"Development Process","text":"<p>We use GitHub to host code, to track issues and feature requests, as well as accept pull requests.</p>"},{"location":"development/contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository and create your branch from <code>main</code>.</li> <li>If you've added code that should be tested, add tests.</li> <li>If you've changed APIs, update the documentation.</li> <li>Ensure the test suite passes.</li> <li>Make sure your code lints.</li> <li>Submit your pull request!</li> </ol>"},{"location":"development/contributing/#setting-up-development-environment","title":"Setting Up Development Environment","text":"<pre><code># Clone the repository\ngit clone https://github.com/yourusername/pynector.git\ncd pynector\n\n# Create and activate a virtual environment (optional but recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows, use: venv\\Scripts\\activate\n\n# Install dependencies with development extras\nuv pip install -e \".[dev]\"\n\n# Install pre-commit hooks\npre-commit install\n</code></pre>"},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with coverage\nuv run pytest --cov=src/pynector\n\n# Run specific test files\nuv run pytest tests/test_client.py\n</code></pre>"},{"location":"development/contributing/#coding-standards","title":"Coding Standards","text":"<ul> <li>We follow PEP 8 for Python code style.</li> <li>Use Black for code formatting.</li> <li>Sort imports using isort.</li> </ul>"},{"location":"development/contributing/#documentation","title":"Documentation","text":"<ul> <li>We use MkDocs with Material theme for documentation.</li> <li>Please update documentation for any changes that affect public APIs.</li> <li>To build and preview the documentation:</li> </ul> <pre><code># Install documentation dependencies\nuv pip install -e \".[docs]\"\n\n# Serve documentation locally\nmkdocs serve\n</code></pre>"},{"location":"development/contributing/#golden-path-workflow","title":"Golden Path Workflow","text":"<p>Pynector follows a standardized development workflow:</p> <ol> <li>Research: For significant features, start with a research report</li> <li>Spec: Create a technical design specification</li> <li>Plan + Tests: Create an implementation plan and tests</li> <li>Code + Green tests: Implement according to the plan</li> <li>Commit: Using conventional commits</li> <li>PR/CI: Submit PR and ensure CI checks pass</li> <li>Review: Code review process</li> <li>Documentation: Update documentation</li> <li>Merge &amp; clean: Merge PR and clean up branch</li> </ol> <p>For more details, see our Golden Path documentation.</p>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the project's MIT License.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Pynector is available on PyPI and can be installed using pip or other Python package managers.</p>"},{"location":"getting-started/installation/#basic-installation","title":"Basic Installation","text":"<pre><code>pip install pynector\n</code></pre>"},{"location":"getting-started/installation/#using-uv","title":"Using uv","text":"<pre><code>uv pip install pynector\n</code></pre>"},{"location":"getting-started/installation/#with-optional-dependencies","title":"With Optional Dependencies","text":"<p>Pynector offers several optional dependency groups that you can install based on your needs:</p>"},{"location":"getting-started/installation/#observability-support","title":"Observability Support","text":"<pre><code>pip install \"pynector[observability]\"\n</code></pre>"},{"location":"getting-started/installation/#tracing-with-zipkin","title":"Tracing with Zipkin","text":"<pre><code>pip install \"pynector[zipkin]\"\n</code></pre>"},{"location":"getting-started/installation/#tracing-with-otlp","title":"Tracing with OTLP","text":"<pre><code>pip install \"pynector[otlp]\"\n</code></pre>"},{"location":"getting-started/installation/#sdk-support-openai-anthropic","title":"SDK Support (OpenAI, Anthropic)","text":"<pre><code>pip install \"pynector[sdk]\"\n</code></pre>"},{"location":"getting-started/installation/#documentation-tools","title":"Documentation Tools","text":"<pre><code>pip install \"pynector[docs]\"\n</code></pre>"},{"location":"getting-started/installation/#all-optional-dependencies","title":"All Optional Dependencies","text":"<p>To install Pynector with all optional dependencies:</p> <pre><code>pip install \"pynector[all]\"\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For development, you might want to install in editable mode:</p> <pre><code>git clone https://github.com/ohdearquant/pynector.git\ncd pynector\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<p>Pynector requires Python 3.9 or later.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide will help you get started with Pynector quickly, showing you how to set up a basic client and make requests.</p>"},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom pynector.client import Client\nfrom pynector.config import ClientConfig\n\nasync def main():\n    # Create a client configuration\n    config = ClientConfig(\n        base_url=\"https://api.example.com\",\n        timeout=30.0,  # 30 seconds timeout\n    )\n\n    # Initialize a client\n    client = Client(config)\n\n    # Make a request\n    response = await client.get(\"/endpoint\")\n\n    # Process the response\n    if response.is_success:\n        data = response.json()\n        print(f\"Received data: {data}\")\n    else:\n        print(f\"Request failed with status: {response.status_code}\")\n        print(f\"Error: {response.text}\")\n\n    # Don't forget to close the client when done\n    await client.close()\n\n# Run the async main function\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/quick-start/#using-http-transport","title":"Using HTTP Transport","text":"<p>Pynector uses HTTP transport by default, but you can explicitly configure it:</p> <pre><code>from pynector.client import Client\nfrom pynector.config import ClientConfig\nfrom pynector.transport.http import HTTPTransport\n\nasync def main():\n    # Create a client with HTTP transport explicitly\n    config = ClientConfig(\n        base_url=\"https://api.example.com\",\n        transport=HTTPTransport(),\n    )\n\n    client = Client(config)\n    # ... use client as before\n</code></pre>"},{"location":"getting-started/quick-start/#working-with-sdk-adapters","title":"Working with SDK Adapters","text":"<p>If you're working with APIs that have Python SDKs, you can use Pynector's SDK transport:</p> <pre><code>from pynector.client import Client\nfrom pynector.config import ClientConfig\nfrom pynector.transport.sdk import SDKTransport\nfrom pynector.transport.sdk.adapter import OpenAIAdapter\n\nasync def main():\n    # Create an SDK transport with the OpenAI adapter\n    transport = SDKTransport(adapter=OpenAIAdapter(api_key=\"your-api-key\"))\n\n    # Create client configuration\n    config = ClientConfig(\n        transport=transport,\n    )\n\n    client = Client(config)\n\n    # Now you can use the client with the SDK adapter\n    response = await client.post(\n        \"/completions\", \n        json={\n            \"model\": \"gpt-4\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, AI!\"}]\n        }\n    )\n\n    if response.is_success:\n        completion = response.json()\n        print(f\"AI response: {completion['choices'][0]['message']['content']}\")\n</code></pre>"},{"location":"getting-started/quick-start/#error-handling","title":"Error Handling","text":"<p>Pynector provides comprehensive error handling:</p> <pre><code>from pynector.client import Client\nfrom pynector.config import ClientConfig\nfrom pynector.errors import RequestError, ResponseError\n\nasync def main():\n    client = Client(ClientConfig(base_url=\"https://api.example.com\"))\n\n    try:\n        response = await client.get(\"/endpoint\")\n        data = response.json()\n\n    except RequestError as e:\n        # Handle connection errors, timeouts, etc.\n        print(f\"Request failed: {e}\")\n\n    except ResponseError as e:\n        # Handle server errors, bad responses, etc.\n        print(f\"Response error: {e}\")\n        print(f\"Status code: {e.status_code}\")\n\n    except Exception as e:\n        # Handle other errors\n        print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"getting-started/quick-start/#advanced-concurrency","title":"Advanced Concurrency","text":"<p>Pynector supports various concurrency patterns:</p> <pre><code>from pynector.client import Client\nfrom pynector.config import ClientConfig\nfrom pynector.concurrency.patterns import gather, race\n\nasync def main():\n    client = Client(ClientConfig(base_url=\"https://api.example.com\"))\n\n    # Make multiple requests in parallel\n    responses = await gather(\n        client.get(\"/endpoint1\"),\n        client.get(\"/endpoint2\"),\n        client.get(\"/endpoint3\"),\n    )\n\n    # Use the first response that completes (race pattern)\n    first_response = await race(\n        client.get(\"/fast-but-unreliable\"),\n        client.get(\"/slow-but-reliable\"),\n    )\n</code></pre> <p>For more detailed information, check out the following sections:</p> <ul> <li>Client Documentation</li> <li>Transport Overview</li> <li>Concurrency Patterns</li> <li>Observability Features</li> </ul>"}]}